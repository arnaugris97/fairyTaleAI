{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d14c95cb-d586-4f80-98d5-c2ce1bdb2cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "#from wordPieceTokenizer import WordPieceTokenizer\n",
    "import json\n",
    "import re\n",
    "import urllib.request\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(0)\n",
    "import pandas as pd\n",
    "def load_separate_and_clean_stories(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    stories = content.split('\\n\\n\\n\\n')\n",
    "\n",
    "    cleaned_stories = []\n",
    "    for story in stories:\n",
    "        cleaned_story = re.sub(r'\\n\\s*\\n', '\\n', story.strip())\n",
    "        cleaned_stories.append(cleaned_story)\n",
    "    \n",
    "    return cleaned_stories\n",
    "\n",
    "def separate_sentences(text):\n",
    "    text = text.replace('...','#^')\n",
    "    text = text.replace('.','~.')\n",
    "    text = text.replace('?','@?')\n",
    "    text = text.replace('!','%!')\n",
    "    \n",
    "    b = re.split('[.?!^]' , text)                                                                                                                                                                                                                                                                                                                                          \n",
    "    c = [w.replace('~', '.') for w in b]\n",
    "    c = [w.replace('@', '?') for w in c]\n",
    "    c = [w.replace('#', '...') for w in c]\n",
    "    c = [w.replace('%', '!') for w in c]\n",
    "    \n",
    "    return(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021a4af4-ee76-4231-804d-35bd6fcdad35",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a811d06-8e85-4c68-8048-10da670cf852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPieceTokenizer():\n",
    "    def __init__(self, vocab_size=10000):\n",
    "        self.vocab = {}\n",
    "        self.word_freqs = {}\n",
    "        self.vocab_size = vocab_size\n",
    "        self.unk_token = \"[UNK]\"\n",
    "        self.aps_token = \"[APS]\"\n",
    "        self.space_token = \"[SPACE]\"\n",
    "        self.brk_token = \"[BRK]\"\n",
    "        self.sep_token = \"[SEP]\"\n",
    "        self.cls_token = \"[CLS]\"\n",
    "        self.pad_token = \"[PAD]\"\n",
    "        self.mask_token = \"[MASK]\"\n",
    "        self.wordpieces_prefix =\"##\"\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "\n",
    "   \n",
    "    def fit(self, text):\n",
    "        # Count word frequencies\n",
    "        text = re.sub(r'\\n+', ' ' + self.brk_token + ' ', text)\n",
    "        text = re.sub(r\"\\s'\\s\", self.space_token + self.aps_token + self.space_token, text)\n",
    "        text = re.sub(r\"\\s'\", self.space_token + self.aps_token, text)\n",
    "        text = re.sub(r\"'\\s\",  self.aps_token + self.space_token, text)\n",
    "        # Change charcater ' to [APS]\n",
    "        text = re.sub(r'\\'', self.aps_token, text)\n",
    "        words = re.findall(r'\\w+[\\w.,;!?\\'\\\"-]*|[\\.,;!?\\'\\\"-]+', text)\n",
    "        \n",
    "        self.word_freqs = Counter(words)\n",
    "\n",
    "        alphabet = []\n",
    "        for word in self.word_freqs.keys():\n",
    "            if word == self.brk_token or word == self.aps_token or word == self.space_token:\n",
    "                continue \n",
    "            # Add the first letter of the word to the alphabet if not exists\n",
    "            if word[0] not in alphabet:\n",
    "                alphabet.append(word[0])\n",
    "            # Add the rest of the letters to the alphabet if not exist with a prefix\n",
    "            for letter in word[1:]:\n",
    "                if f\"##{letter}\" not in alphabet:\n",
    "                    alphabet.append(f\"##{letter}\")\n",
    "\n",
    "        alphabet.sort()\n",
    "        \n",
    "        # Add special tokens to the vocabulary plus the created alphabet\n",
    "        self.vocab = [self.unk_token, self.cls_token, self.sep_token, self.space_token, self.pad_token, self.mask_token, self.brk_token, self.aps_token ] + alphabet.copy()\n",
    "        # Create a dictionary with all words and all splitted characters\n",
    "        splits = {\n",
    "            word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "            for word in self.word_freqs.keys()\n",
    "        }\n",
    "        \n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            scores = self._compute_pair_scores(splits)\n",
    "            if not scores:\n",
    "                break\n",
    "            best_pair, max_score = \"\", None\n",
    "            for pair, score in scores.items():\n",
    "                if max_score is None or max_score < score:\n",
    "                    best_pair = pair\n",
    "                    max_score = score\n",
    "            \n",
    "            splits = self._merge_pair(*best_pair, splits)\n",
    "            new_token = (\n",
    "                best_pair[0] + best_pair[1][2:]\n",
    "                if best_pair[1].startswith(\"##\")\n",
    "                else best_pair[0] + best_pair[1]\n",
    "            )\n",
    "            self.vocab.append(new_token)\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: word for idx, word in enumerate(self.vocab)}\n",
    "        print(self.vocab)\n",
    "        print(len(self.vocab))\n",
    "        print(self.word_freqs)\n",
    "    \n",
    "    \n",
    "    def encode(self, text):\n",
    "        # Normalize and split the text\n",
    "        text = re.sub(r'\\n+', ' ' + self.brk_token + ' ', text)\n",
    "        text = re.sub(r\"\\s'\\s\", self.space_token + self.aps_token + self.space_token, text)\n",
    "        text = re.sub(r\"\\s'\", self.space_token + self.aps_token, text)\n",
    "        text = re.sub(r\"'\\s\",  self.aps_token + self.space_token, text)\n",
    "        # Change charcater ' to [APS] and\n",
    "        text = re.sub(r'\\'', self.aps_token, text)\n",
    "        pattern = r'\\w+[\\w.,;!?\\'\\\"-]*|[\\.,;!?\\'\\\"-]+|(?:' + re.escape(self.brk_token) + r'|' + re.escape(self.aps_token) + r'|' + re.escape(self.space_token) + r')'\n",
    "        words = re.findall(pattern, text)\n",
    "        \n",
    "        # Tokenize into words and subwords\n",
    "        tokens = []\n",
    "        for word in words:\n",
    "            if word in self.vocab:\n",
    "                tokens.append(word)\n",
    "            else:\n",
    "                sub_tokens = self.tokenize_word(word)\n",
    "                tokens.extend(sub_tokens)\n",
    "    \n",
    "        \n",
    "        # Convert tokens to ids\n",
    "        token_ids = []\n",
    "        token_ids.extend(self.word2idx[token] for token in tokens if token in self.word2idx)\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def tokenize_word(self, word):\n",
    "        if word == self.brk_token:\n",
    "            return [self.brk_token]\n",
    "        if word == self.aps_token:\n",
    "            return [self.aps_token]\n",
    "        if word == self.space_token:\n",
    "            return [self.space_token]\n",
    "        \n",
    "        subwords = []\n",
    "        start = 0\n",
    "        while start < len(word):\n",
    "            match = False\n",
    "            for end in range(len(word), start, -1):\n",
    "                subword = word[start:end]\n",
    "                if start > 0:\n",
    "                    subword = \"##\" + subword\n",
    "                if subword in self.vocab:\n",
    "                    subwords.append(subword)\n",
    "                    start = end\n",
    "                    match = True\n",
    "                    break\n",
    "            if not match:  # No subword match found\n",
    "                subwords.append(self.unk_token)\n",
    "                break\n",
    "        return subwords\n",
    "    \n",
    "    def add_special_tokens(self, token_ids1, token_ids2, max_length=60):\n",
    "        tokens_with_special_tokens  = [self.word2idx[self.cls_token]] + token_ids1 + [self.word2idx[self.sep_token]] + token_ids2 + [self.word2idx[self.sep_token]]\n",
    "        # Create attention mask\n",
    "        attention_mask = [1] * len(tokens_with_special_tokens)\n",
    "\n",
    "        # Create token segment type ids\n",
    "        token_type_ids = [0] * (len(token_ids1) + 2) + [1] * (len(token_ids2) + 1)\n",
    "        \n",
    "\n",
    "        padded_token_ids = tokens_with_special_tokens + [self.word2idx[self.pad_token]] * (max_length - len(tokens_with_special_tokens))\n",
    "        attention_mask = attention_mask + [0] * (max_length - len(attention_mask))\n",
    "        token_type_ids = token_type_ids + [0] * (max_length - len(token_type_ids))\n",
    "        \n",
    "        return padded_token_ids, attention_mask, token_type_ids\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        tokens = [self.idx2word[index] for index in indices]\n",
    "        # Split the text by the first sep_token\n",
    "        sep_index = tokens.index(self.sep_token)\n",
    "        sentence1 = tokens[1:sep_index]\n",
    "        sentence2 = tokens[sep_index + 1:]\n",
    "        text1 = ''\n",
    "        text2 = ''\n",
    "        # Perform for loop for both sentences at the same time\n",
    "        \n",
    "\n",
    "        for token in sentence1:\n",
    "            if token.startswith(self.wordpieces_prefix):\n",
    "                # Remove the '##' prefix and concatenate without space\n",
    "                text1 += token[2:]\n",
    "            elif token in [self.unk_token, self.cls_token, self.sep_token, self.pad_token, self.mask_token]:\n",
    "                # Skip special tokens if desired, or handle them differently\n",
    "                continue\n",
    "            elif token == self.aps_token:\n",
    "                # Replace [APS] with a ' character\n",
    "                text1 += \"'\"\n",
    "            elif token == self.space_token:\n",
    "                # Replace [SPACE] with a space character\n",
    "                text1 += ' '\n",
    "            elif token == self.brk_token:\n",
    "                # Replace [BRK] with a newline character\n",
    "                text1 += '\\n'\n",
    "            else:\n",
    "                # Add a space before the token if it's not the first token and the last character isn't a newline\n",
    "                if text1 and not text1.endswith('\\n') and not text1.endswith(\"'\"):\n",
    "                    text1 += ' '\n",
    "                text1 += token\n",
    "\n",
    "        for token in sentence1:\n",
    "            if token.startswith(self.wordpieces_prefix):\n",
    "                # Remove the '##' prefix and concatenate without space\n",
    "                text2 += token[2:]\n",
    "            elif token in [self.unk_token, self.cls_token, self.sep_token, self.pad_token, self.mask_token]:\n",
    "                # Skip special tokens if desired, or handle them differently\n",
    "                continue\n",
    "            elif token == self.aps_token:\n",
    "                # Replace [APS] with a ' character\n",
    "                text2 += \"'\"\n",
    "            elif token == self.space_token:\n",
    "                # Replace [SPACE] with a space character\n",
    "                text2 += ' '\n",
    "            elif token == self.brk_token:\n",
    "                # Replace [BRK] with a newline character\n",
    "                text2 += '\\n'\n",
    "            else:\n",
    "                # Add a space before the token if it's not the first token and the last character isn't a newline\n",
    "                if text2 and not text2.endswith('\\n') and not text2.endswith(\"'\"):\n",
    "                    text2 += ' '\n",
    "                text2 += token\n",
    "\n",
    "        return text1, text2\n",
    "    \n",
    "    def save(self, path):\n",
    "       with open(path, 'w') as f:\n",
    "            json.dump({\n",
    "                'vocab': self.vocab,\n",
    "                'word_freqs': self.word_freqs,\n",
    "                'word2idx': self.word2idx,\n",
    "                'idx2word': self.idx2word,\n",
    "            }, f, ensure_ascii=False)\n",
    "\n",
    "    def load(self, path):\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            self.vocab =  data['vocab']\n",
    "            self.vocab_size = len(self.vocab)\n",
    "            self.word_freqs =  {k: int(v) for k, v in data['word_freqs'].items()}\n",
    "            self.word2idx =  {k: int(v) for k, v in data['word2idx'].items()}\n",
    "            self.idx2word = {int(k): v for k, v in data['idx2word'].items()}\n",
    "            \n",
    "        \n",
    "    def _compute_pair_scores(self, splits):\n",
    "        letter_freqs = defaultdict(int)\n",
    "        pair_freqs = defaultdict(int)\n",
    "        # Compute the frequency of each letter and pair of consecutive letters\n",
    "        for word, freq in self.word_freqs.items():\n",
    "            split = splits[word]\n",
    "            if len(split) == 1:\n",
    "                letter_freqs[split[0]] += freq\n",
    "                continue\n",
    "            for i in range(len(split) - 1):\n",
    "                pair = (split[i], split[i + 1])\n",
    "                letter_freqs[split[i]] += freq\n",
    "                pair_freqs[pair] += freq\n",
    "            letter_freqs[split[-1]] += freq\n",
    "\n",
    "        # Compute the score of each pair (pair frequency / (letter1 frequency * letter2 frequency)\n",
    "        scores = {\n",
    "            pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "            for pair, freq in pair_freqs.items()\n",
    "        }\n",
    "        return scores\n",
    "    \n",
    "    def _merge_pair(self, a, b, splits):\n",
    "        for word in self.word_freqs:\n",
    "            split = splits[word]\n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == a and split[i + 1] == b:\n",
    "                    merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[word] = split\n",
    "        return splits\n",
    "\n",
    "def mask_tokens(token_ids, tokenizer):\n",
    "    # Mask 15% of the tokens\n",
    "    masked_indices = set()\n",
    "    # 15% of significant tokens, different to [CLS], [SEP], and [PAD]\n",
    "    significant_tokens = [token for token in token_ids if token not in [tokenizer.word2idx[tokenizer.cls_token], tokenizer.word2idx[tokenizer.sep_token], tokenizer.word2idx[tokenizer.pad_token]]]\n",
    "    num_masked = max(1, int(len(significant_tokens) * 0.15))\n",
    "    while len(masked_indices) < num_masked:\n",
    "        index = random.randint(1, len(token_ids) - 2)\n",
    "        if index in masked_indices:\n",
    "            continue\n",
    "        token = token_ids[index]\n",
    "        if token in [tokenizer.word2idx[tokenizer.cls_token], tokenizer.word2idx[tokenizer.sep_token], tokenizer.word2idx[tokenizer.pad_token]]:\n",
    "            continue\n",
    "        token_ids[index] = tokenizer.word2idx[tokenizer.mask_token]\n",
    "        masked_indices.add(index)\n",
    "    \n",
    "    labels = [-100 if i not in masked_indices else token_ids[i] for i in range(len(token_ids))]\n",
    "    return token_ids, labels\n",
    "\n",
    "\n",
    "def load_separate_and_clean_stories(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    stories = content.split('\\n\\n\\n\\n')\n",
    "\n",
    "    cleaned_stories = []\n",
    "    for story in stories:\n",
    "        cleaned_story = re.sub(r'\\n\\s*\\n', '\\n', story.strip())\n",
    "        cleaned_stories.append(cleaned_story)\n",
    "    \n",
    "    return cleaned_stories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3409336-7460-4b26-a0f2-03c9690b0764",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## V.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e1d6409-bc2d-480b-b9bf-8e856523a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dict_,mask=False):\n",
    "        super().__init__()\n",
    "        self.dict = dict_\n",
    "        self.mask = mask\n",
    "    def __len__(self):\n",
    "        return len(self.dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        text = self.dict[str(idx)]['text']\n",
    "\n",
    "        tokens = torch.tensor(self.dict[str(idx)]['tokens'])\n",
    "\n",
    "        if self.mask!=False:\n",
    "            idxs = np.linspace(0,len(tokens)-1,len(tokens)).astype(int)\n",
    "            pos = random.choices(idxs, k=int(len(idxs)*0.15))\n",
    "            tokens[pos] = self.mask\n",
    "\n",
    "        return text,tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19421035-60b8-4739-9647-d1ccbc37e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = WordPieceTokenizer()\n",
    "tokenizer.load('wordPieceVocab.json')\n",
    "\n",
    "# Load the dataset\n",
    "dataset_txt = load_separate_and_clean_stories(\"dataset/combined_stories.txt\")\n",
    "\n",
    "dict_ = {}\n",
    "for i in range(len(dataset_txt)//20):\n",
    "    dict_[i] = {'text':dataset_txt[i],\n",
    "                'tokens':tokenizer.encode_n(dataset_txt[i])\n",
    "                 }\n",
    "\n",
    "with open(\"dataset/dataset_dict.json\", \"w\") as outfile: \n",
    "    json.dump(dict_, outfile)  # We can read this file to avoid computing again the dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08cfb7fe-7233-4d53-946e-9faf41e5d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/dataset_dict.json') as json_file:\n",
    "    dict_ = json.load(json_file)\n",
    "    \n",
    "complete_dataloader = MyDataset(dict_,tokenizer.word2idx[\"[MASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3666d63c-d386-400c-b8b1-418604d3f754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1597"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(complete_dataloader.__getitem__(0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52fc7626-ea71-4745-a77d-f151aa18a5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(complete_dataloader.__getitem__(0)[1]==4)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb7f01f-7692-4bbe-aaa3-2bd612a3fc8d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## V.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebbd024c-9690-4542-896d-1bf9644f30a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset,sentences):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.sentences = sentences\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        title = self.dataset.iloc[idx]['Title']\n",
    "        text = separate_sentences(self.dataset.iloc[idx]['cleaned_story'])\n",
    "        list_sentences = [''.join(map(str, text[i:i+self.sentences])) for i in range(0, len(text), self.sentences)]\n",
    "\n",
    "        return title,text,list_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4c6c6b5-6af7-45be-8c9d-e4a5ef80c160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>cleaned_story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Thumbelina</td>\n",
       "      <td>Once upon a time, in a world of wonder and enc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Star Money</td>\n",
       "      <td>Once upon a time, in a quaint village nestled ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The Twelve Dancing Princesses</td>\n",
       "      <td>In a kingdom where castles touched the clouds ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Elves and the Shoemaker</td>\n",
       "      <td>In a quaint village nestled at the edge of a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Fox and the Cat</td>\n",
       "      <td>Once upon a time, in a lush forest filled with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                          Title  \\\n",
       "0           0                     Thumbelina   \n",
       "1           1                 The Star Money   \n",
       "2           2  The Twelve Dancing Princesses   \n",
       "3           3    The Elves and the Shoemaker   \n",
       "4           4            The Fox and the Cat   \n",
       "\n",
       "                                       cleaned_story  \n",
       "0  Once upon a time, in a world of wonder and enc...  \n",
       "1  Once upon a time, in a quaint village nestled ...  \n",
       "2  In a kingdom where castles touched the clouds ...  \n",
       "3  In a quaint village nestled at the edge of a l...  \n",
       "4  Once upon a time, in a lush forest filled with...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('dataset/merged_stories(1).csv')\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0a03579-b228-448d-a322-272837c5a954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thumbelina\n",
      "60\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "complete_dataloader = MyDataset(dataset,6)\n",
    "print(complete_dataloader.__getitem__(0)[0])\n",
    "print(len(complete_dataloader.__getitem__(0)[1]))\n",
    "print(len(complete_dataloader.__getitem__(0)[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "04128d6c-eaf9-4357-90e6-e92c2f7a4bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My text...', 'My text.', ' My text!', ' My text?', '']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 'My text...My text. My text! My text?'\n",
    "separate_sentences(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64362273-090a-48ec-a487-3e715dafa6f8",
   "metadata": {},
   "source": [
    "## V.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3319081b-a5a2-4c7e-a0ce-37d5437509bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset,sentences):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.sentences = sentences\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        title = self.dataset.iloc[idx]['Title']\n",
    "        text = separate_sentences(self.dataset.iloc[idx]['cleaned_story'])\n",
    "        list_sentences = [''.join(map(str, text[i:i+self.sentences])) for i in range(0, len(text), self.sentences)]\n",
    "        it = random.randint(0,len(list_sentences)-2)\n",
    "        sentence = list_sentences[it]\n",
    "\n",
    "        if random.random()<0.5:\n",
    "            next_sentence = list_sentences[it+1]\n",
    "            is_next = True\n",
    "            \n",
    "        else:\n",
    "            idx2 = idx\n",
    "            while idx2 == idx:\n",
    "                idx2 = random.randint(0,len(self.dataset)-1)\n",
    "            text2 = separate_sentences(self.dataset.iloc[idx2]['cleaned_story'])\n",
    "            list_sentences2 = [''.join(map(str, text2[i:i+self.sentences])) for i in range(0, len(text2), self.sentences)]\n",
    "            it = random.randint(0,len(list_sentences2)-1)\n",
    "            next_sentence = list_sentences2[it]\n",
    "            \n",
    "            is_next = False\n",
    "        return title,sentence,next_sentence,is_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1af4ecad-31d7-4073-bd4a-ea022800a423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>cleaned_story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Thumbelina</td>\n",
       "      <td>Once upon a time, in a world of wonder and enc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Star Money</td>\n",
       "      <td>Once upon a time, in a quaint village nestled ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The Twelve Dancing Princesses</td>\n",
       "      <td>In a kingdom where castles touched the clouds ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Elves and the Shoemaker</td>\n",
       "      <td>In a quaint village nestled at the edge of a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Fox and the Cat</td>\n",
       "      <td>Once upon a time, in a lush forest filled with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                          Title  \\\n",
       "0           0                     Thumbelina   \n",
       "1           1                 The Star Money   \n",
       "2           2  The Twelve Dancing Princesses   \n",
       "3           3    The Elves and the Shoemaker   \n",
       "4           4            The Fox and the Cat   \n",
       "\n",
       "                                       cleaned_story  \n",
       "0  Once upon a time, in a world of wonder and enc...  \n",
       "1  Once upon a time, in a quaint village nestled ...  \n",
       "2  In a kingdom where castles touched the clouds ...  \n",
       "3  In a quaint village nestled at the edge of a l...  \n",
       "4  Once upon a time, in a lush forest filled with...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('/Users/francesccarandellverdaguer/fairyTaleAI/dataset/merged_stories_full.csv')\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd520e5f-3b30-4e46-adfa-9749f1ffe13a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty range for randrange() (0, 0, 0)\n",
      "Icarus and the Wax Wings: A Greek Myth\n"
     ]
    }
   ],
   "source": [
    "complete_dataloader = Custom_Dataset(dataset,2)\n",
    "for i in range(len(dataset)):\n",
    "    try:\n",
    "        complete_dataloader.__getitem__(i)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(dataset.iloc[i].Title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d44d8b-a705-4d64-8fa3-a4fe6d3e845e",
   "metadata": {},
   "source": [
    "### DATASET ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49c411b3-0f74-48c4-bd2f-92631f73b51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't Let the Pigeon Drive the Bus! 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m list_sentences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, text[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m])) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(text), \u001b[38;5;241m2\u001b[39m)]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(list_sentences)):\n\u001b[0;32m---> 10\u001b[0m     enc \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_sentences\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(enc)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m512\u001b[39m:\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(dataset\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;28mstr\u001b[39m(i))\n",
      "Cell \u001b[0;32mIn[4], line 94\u001b[0m, in \u001b[0;36mWordPieceTokenizer.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     92\u001b[0m         tokens\u001b[38;5;241m.\u001b[39mappend(word)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m         sub_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m         tokens\u001b[38;5;241m.\u001b[39mextend(sub_tokens)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Convert tokens to ids\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 120\u001b[0m, in \u001b[0;36mWordPieceTokenizer.tokenize_word\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    119\u001b[0m     subword \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m##\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m subword\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subword \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab:\n\u001b[1;32m    121\u001b[0m     subwords\u001b[38;5;241m.\u001b[39mappend(subword)\n\u001b[1;32m    122\u001b[0m     start \u001b[38;5;241m=\u001b[39m end\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('/Users/francesccarandellverdaguer/fairyTaleAI/dataset/merged_stories_full.csv')\n",
    "dataset.head(5)\n",
    "tokenizer = WordPieceTokenizer()\n",
    "tokenizer.load('/Users/francesccarandellverdaguer/fairyTaleAI/tokenizer/wordPieceVocab.json')\n",
    "\n",
    "for idx in range(len(dataset)):\n",
    "    text = separate_sentences(dataset.iloc[idx]['cleaned_story'])\n",
    "    list_sentences = [''.join(map(str, text[i:i+2])) for i in range(0, len(text), 2)]\n",
    "    for i in range(len(list_sentences)):\n",
    "        enc = tokenizer.encode(list_sentences[i])\n",
    "\n",
    "        if len(enc)>512:\n",
    "            print(dataset.iloc[idx]['Title'],str(i))\n",
    "       # if len(enc):\n",
    "        #    print(dataset.iloc[idx]['Title'],str(i),str(len(enc)))\n",
    "    if len(list_sentences)<=2:\n",
    "        print(dataset.iloc[idx]['Title'],str(len(list_sentences)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f392ff2-bb7e-41c1-b5ba-dc09c785e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = dataset[dataset.Title==\"The Twelve Dancing Princesses\"]\n",
    "text = separate_sentences(sub.iloc[0]['cleaned_story'])\n",
    "list_sentences = [''.join(map(str, text[i:i+2])) for i in range(0, len(text), 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d71bb443-1889-4b0d-bbdd-5b292fbe414b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In a kingdom where castles touched the clouds and gardens bloomed with magic, there lived a kind king with twelve daughters. The princesses were known far and wide for their beauty, grace, and love for dancing.',\n",
       " ' But a mysterious secret lay hidden behind their enchanting smiles. Each night, after the kingdom fell into slumber, the princesses would gather in their elegant chamber.',\n",
       " ' With a flick of their fingers, the floor would transform into a portal to a magical world. In that world, they danced the night away, their shoes sparkling like stars as they twirled beneath the moonlight.',\n",
       " \" The king, concerned about his daughters' tired eyes and worn-out shoes, announced a grand challenge. He invited brave princes from across the land to solve the mystery of the dancing princesses.\",\n",
       " ' The one who succeeded would choose a princess to marry and inherit the kingdom. Prince Leo, known for his wit and courage, arrived at the palace determined to uncover the secret.',\n",
       " ' The princesses welcomed him with warmth and hospitality, and Leo marveled at their beauty as they danced at the royal ball. However, the secret of the enchanted shoes remained elusive.',\n",
       " ' Night after night, Prince Leo pretended to sleep while keeping a watchful eye on the princesses. Just as the clock struck midnight, the princesses rose from their beds and stepped into the magical world.',\n",
       " ' With a heart filled with curiosity, Prince Leo followed them through the portal, his steps silent as a whisper. He marveled at the world he entered—a garden of silver trees, a river of liquid diamonds, and a ballroom adorned with moonbeams.',\n",
       " ' The princesses danced with joy, their laughter echoing through the night. Prince Leo watched in wonder, realizing that their secret world was a place of pure happiness and magic.',\n",
       " ' As the night waned, the princesses returned to the palace, unaware of their visitor. With the morning sun, Prince Leo rushed to the king, his heart full of understanding.',\n",
       " \" He explained the enchantment and presented the king with a bouquet of enchanted roses, their petals woven with moonlight. The king was moved by Leo's wisdom and kindness.\",\n",
       " ' He declared that Leo had solved the mystery and deserved to choose a princess to marry. Leo looked into the eyes of the youngest princess, Lily, her spirit as pure as a morning breeze.',\n",
       " \" The king's kingdom celebrated with a grand wedding, and as Lily and Leo danced their first dance, the magic of the enchanted shoes filled the air. But this time, the enchantment was shared with everyone, a gift of joy and wonder that brought laughter to the hearts of all.\",\n",
       " ' And so, in a kingdom where castles touched the clouds and gardens bloomed with magic, the mystery of the twelve dancing princesses was solved by the bravery and understanding of a prince named Leo. Their enchanted shoes continued to dance, not only in the hidden world but also in the hearts of those who believed in the magic of love and unity.',\n",
       " '']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d12f16-9847-41bd-b995-920dc2ae40f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
