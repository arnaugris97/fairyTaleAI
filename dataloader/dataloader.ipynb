{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d14c95cb-d586-4f80-98d5-c2ce1bdb2cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "#from wordPieceTokenizer import WordPieceTokenizer\n",
    "import json\n",
    "import re\n",
    "import urllib.request\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(0)\n",
    "import pandas as pd\n",
    "def load_separate_and_clean_stories(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    stories = content.split('\\n\\n\\n\\n')\n",
    "\n",
    "    cleaned_stories = []\n",
    "    for story in stories:\n",
    "        cleaned_story = re.sub(r'\\n\\s*\\n', '\\n', story.strip())\n",
    "        cleaned_stories.append(cleaned_story)\n",
    "    \n",
    "    return cleaned_stories\n",
    "\n",
    "def separate_sentences(text):\n",
    "    text = text.replace('...','#^')\n",
    "    text = text.replace('.','~.')\n",
    "    text = text.replace('?','@?')\n",
    "    text = text.replace('!','%!')\n",
    "    \n",
    "    b = re.split('[.?!^]' , text)                                                                                                                                                                                                                                                                                                                                          \n",
    "    c = [w.replace('~', '.') for w in b]\n",
    "    c = [w.replace('@', '?') for w in c]\n",
    "    c = [w.replace('#', '...') for w in c]\n",
    "    c = [w.replace('%', '!') for w in c]\n",
    "    \n",
    "    return(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021a4af4-ee76-4231-804d-35bd6fcdad35",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a811d06-8e85-4c68-8048-10da670cf852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPieceTokenizer():\n",
    "    def __init__(self, vocab_size=10000):\n",
    "        self.vocab = {}\n",
    "        self.word_freqs = {}\n",
    "        self.vocab_size = vocab_size\n",
    "        self.unk_token = \"[UNK]\"\n",
    "        self.aps_token = \"[APS]\"\n",
    "        self.space_token = \"[SPACE]\"\n",
    "        self.brk_token = \"[BRK]\"\n",
    "        self.sep_token = \"[SEP]\"\n",
    "        self.cls_token = \"[CLS]\"\n",
    "        self.pad_token = \"[PAD]\"\n",
    "        self.mask_token = \"[MASK]\"\n",
    "        self.wordpieces_prefix =\"##\"\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "\n",
    "   \n",
    "    def fit(self, text):\n",
    "        # Count word frequencies\n",
    "        text = re.sub(r'\\n+', ' ' + self.brk_token + ' ', text)\n",
    "        text = re.sub(r\"\\s'\\s\", self.space_token + self.aps_token + self.space_token, text)\n",
    "        text = re.sub(r\"\\s'\", self.space_token + self.aps_token, text)\n",
    "        text = re.sub(r\"'\\s\",  self.aps_token + self.space_token, text)\n",
    "        # Change charcater ' to [APS]\n",
    "        text = re.sub(r'\\'', self.aps_token, text)\n",
    "        words = re.findall(r'\\w+[\\w.,;!?\\'\\\"-]*|[\\.,;!?\\'\\\"-]+', text)\n",
    "        \n",
    "        self.word_freqs = Counter(words)\n",
    "\n",
    "        alphabet = []\n",
    "        for word in self.word_freqs.keys():\n",
    "            if word == self.brk_token or word == self.aps_token or word == self.space_token:\n",
    "                continue \n",
    "            # Add the first letter of the word to the alphabet if not exists\n",
    "            if word[0] not in alphabet:\n",
    "                alphabet.append(word[0])\n",
    "            # Add the rest of the letters to the alphabet if not exist with a prefix\n",
    "            for letter in word[1:]:\n",
    "                if f\"##{letter}\" not in alphabet:\n",
    "                    alphabet.append(f\"##{letter}\")\n",
    "\n",
    "        alphabet.sort()\n",
    "        \n",
    "        # Add special tokens to the vocabulary plus the created alphabet\n",
    "        self.vocab = [self.unk_token, self.cls_token, self.sep_token, self.space_token, self.pad_token, self.mask_token, self.brk_token, self.aps_token ] + alphabet.copy()\n",
    "        # Create a dictionary with all words and all splitted characters\n",
    "        splits = {\n",
    "            word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "            for word in self.word_freqs.keys()\n",
    "        }\n",
    "        \n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            scores = self._compute_pair_scores(splits)\n",
    "            if not scores:\n",
    "                break\n",
    "            best_pair, max_score = \"\", None\n",
    "            for pair, score in scores.items():\n",
    "                if max_score is None or max_score < score:\n",
    "                    best_pair = pair\n",
    "                    max_score = score\n",
    "            \n",
    "            splits = self._merge_pair(*best_pair, splits)\n",
    "            new_token = (\n",
    "                best_pair[0] + best_pair[1][2:]\n",
    "                if best_pair[1].startswith(\"##\")\n",
    "                else best_pair[0] + best_pair[1]\n",
    "            )\n",
    "            self.vocab.append(new_token)\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: word for idx, word in enumerate(self.vocab)}\n",
    "        print(self.vocab)\n",
    "        print(len(self.vocab))\n",
    "        print(self.word_freqs)\n",
    "    \n",
    "    \n",
    "    def encode(self, text):\n",
    "        # Normalize and split the text\n",
    "        text = re.sub(r'\\n+', ' ' + self.brk_token + ' ', text)\n",
    "        text = re.sub(r\"\\s'\\s\", self.space_token + self.aps_token + self.space_token, text)\n",
    "        text = re.sub(r\"\\s'\", self.space_token + self.aps_token, text)\n",
    "        text = re.sub(r\"'\\s\",  self.aps_token + self.space_token, text)\n",
    "        # Change charcater ' to [APS] and\n",
    "        text = re.sub(r'\\'', self.aps_token, text)\n",
    "        pattern = r'\\w+[\\w.,;!?\\'\\\"-]*|[\\.,;!?\\'\\\"-]+|(?:' + re.escape(self.brk_token) + r'|' + re.escape(self.aps_token) + r'|' + re.escape(self.space_token) + r')'\n",
    "        words = re.findall(pattern, text)\n",
    "        \n",
    "        # Tokenize into words and subwords\n",
    "        tokens = []\n",
    "        for word in words:\n",
    "            if word in self.vocab:\n",
    "                tokens.append(word)\n",
    "            else:\n",
    "                sub_tokens = self.tokenize_word(word)\n",
    "                tokens.extend(sub_tokens)\n",
    "    \n",
    "        \n",
    "        # Convert tokens to ids\n",
    "        token_ids = []\n",
    "        token_ids.extend(self.word2idx[token] for token in tokens if token in self.word2idx)\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def tokenize_word(self, word):\n",
    "        if word == self.brk_token:\n",
    "            return [self.brk_token]\n",
    "        if word == self.aps_token:\n",
    "            return [self.aps_token]\n",
    "        if word == self.space_token:\n",
    "            return [self.space_token]\n",
    "        \n",
    "        subwords = []\n",
    "        start = 0\n",
    "        while start < len(word):\n",
    "            match = False\n",
    "            for end in range(len(word), start, -1):\n",
    "                subword = word[start:end]\n",
    "                if start > 0:\n",
    "                    subword = \"##\" + subword\n",
    "                if subword in self.vocab:\n",
    "                    subwords.append(subword)\n",
    "                    start = end\n",
    "                    match = True\n",
    "                    break\n",
    "            if not match:  # No subword match found\n",
    "                subwords.append(self.unk_token)\n",
    "                break\n",
    "        return subwords\n",
    "    \n",
    "    def add_special_tokens(self, token_ids1, token_ids2, max_length=60):\n",
    "        tokens_with_special_tokens  = [self.word2idx[self.cls_token]] + token_ids1 + [self.word2idx[self.sep_token]] + token_ids2 + [self.word2idx[self.sep_token]]\n",
    "        # Create attention mask\n",
    "        attention_mask = [1] * len(tokens_with_special_tokens)\n",
    "\n",
    "        # Create token segment type ids\n",
    "        token_type_ids = [0] * (len(token_ids1) + 2) + [1] * (len(token_ids2) + 1)\n",
    "        \n",
    "\n",
    "        padded_token_ids = tokens_with_special_tokens + [self.word2idx[self.pad_token]] * (max_length - len(tokens_with_special_tokens))\n",
    "        attention_mask = attention_mask + [0] * (max_length - len(attention_mask))\n",
    "        token_type_ids = token_type_ids + [0] * (max_length - len(token_type_ids))\n",
    "        \n",
    "        return padded_token_ids, attention_mask, token_type_ids\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        tokens = [self.idx2word[index] for index in indices]\n",
    "        # Split the text by the first sep_token\n",
    "        sep_index = tokens.index(self.sep_token)\n",
    "        sentence1 = tokens[1:sep_index]\n",
    "        sentence2 = tokens[sep_index + 1:]\n",
    "        text1 = ''\n",
    "        text2 = ''\n",
    "        # Perform for loop for both sentences at the same time\n",
    "        \n",
    "\n",
    "        for token in sentence1:\n",
    "            if token.startswith(self.wordpieces_prefix):\n",
    "                # Remove the '##' prefix and concatenate without space\n",
    "                text1 += token[2:]\n",
    "            elif token in [self.unk_token, self.cls_token, self.sep_token, self.pad_token, self.mask_token]:\n",
    "                # Skip special tokens if desired, or handle them differently\n",
    "                continue\n",
    "            elif token == self.aps_token:\n",
    "                # Replace [APS] with a ' character\n",
    "                text1 += \"'\"\n",
    "            elif token == self.space_token:\n",
    "                # Replace [SPACE] with a space character\n",
    "                text1 += ' '\n",
    "            elif token == self.brk_token:\n",
    "                # Replace [BRK] with a newline character\n",
    "                text1 += '\\n'\n",
    "            else:\n",
    "                # Add a space before the token if it's not the first token and the last character isn't a newline\n",
    "                if text1 and not text1.endswith('\\n') and not text1.endswith(\"'\"):\n",
    "                    text1 += ' '\n",
    "                text1 += token\n",
    "\n",
    "        for token in sentence1:\n",
    "            if token.startswith(self.wordpieces_prefix):\n",
    "                # Remove the '##' prefix and concatenate without space\n",
    "                text2 += token[2:]\n",
    "            elif token in [self.unk_token, self.cls_token, self.sep_token, self.pad_token, self.mask_token]:\n",
    "                # Skip special tokens if desired, or handle them differently\n",
    "                continue\n",
    "            elif token == self.aps_token:\n",
    "                # Replace [APS] with a ' character\n",
    "                text2 += \"'\"\n",
    "            elif token == self.space_token:\n",
    "                # Replace [SPACE] with a space character\n",
    "                text2 += ' '\n",
    "            elif token == self.brk_token:\n",
    "                # Replace [BRK] with a newline character\n",
    "                text2 += '\\n'\n",
    "            else:\n",
    "                # Add a space before the token if it's not the first token and the last character isn't a newline\n",
    "                if text2 and not text2.endswith('\\n') and not text2.endswith(\"'\"):\n",
    "                    text2 += ' '\n",
    "                text2 += token\n",
    "\n",
    "        return text1, text2\n",
    "    \n",
    "    def save(self, path):\n",
    "       with open(path, 'w') as f:\n",
    "            json.dump({\n",
    "                'vocab': self.vocab,\n",
    "                'word_freqs': self.word_freqs,\n",
    "                'word2idx': self.word2idx,\n",
    "                'idx2word': self.idx2word,\n",
    "            }, f, ensure_ascii=False)\n",
    "\n",
    "    def load(self, path):\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            self.vocab =  data['vocab']\n",
    "            self.vocab_size = len(self.vocab)\n",
    "            self.word_freqs =  {k: int(v) for k, v in data['word_freqs'].items()}\n",
    "            self.word2idx =  {k: int(v) for k, v in data['word2idx'].items()}\n",
    "            self.idx2word = {int(k): v for k, v in data['idx2word'].items()}\n",
    "            \n",
    "        \n",
    "    def _compute_pair_scores(self, splits):\n",
    "        letter_freqs = defaultdict(int)\n",
    "        pair_freqs = defaultdict(int)\n",
    "        # Compute the frequency of each letter and pair of consecutive letters\n",
    "        for word, freq in self.word_freqs.items():\n",
    "            split = splits[word]\n",
    "            if len(split) == 1:\n",
    "                letter_freqs[split[0]] += freq\n",
    "                continue\n",
    "            for i in range(len(split) - 1):\n",
    "                pair = (split[i], split[i + 1])\n",
    "                letter_freqs[split[i]] += freq\n",
    "                pair_freqs[pair] += freq\n",
    "            letter_freqs[split[-1]] += freq\n",
    "\n",
    "        # Compute the score of each pair (pair frequency / (letter1 frequency * letter2 frequency)\n",
    "        scores = {\n",
    "            pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "            for pair, freq in pair_freqs.items()\n",
    "        }\n",
    "        return scores\n",
    "    \n",
    "    def _merge_pair(self, a, b, splits):\n",
    "        for word in self.word_freqs:\n",
    "            split = splits[word]\n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == a and split[i + 1] == b:\n",
    "                    merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[word] = split\n",
    "        return splits\n",
    "\n",
    "def mask_tokens(token_ids, tokenizer):\n",
    "    # Mask 15% of the tokens\n",
    "    masked_indices = set()\n",
    "    # 15% of significant tokens, different to [CLS], [SEP], and [PAD]\n",
    "    significant_tokens = [token for token in token_ids if token not in [tokenizer.word2idx[tokenizer.cls_token], tokenizer.word2idx[tokenizer.sep_token], tokenizer.word2idx[tokenizer.pad_token]]]\n",
    "    num_masked = max(1, int(len(significant_tokens) * 0.15))\n",
    "    while len(masked_indices) < num_masked:\n",
    "        index = random.randint(1, len(token_ids) - 2)\n",
    "        if index in masked_indices:\n",
    "            continue\n",
    "        token = token_ids[index]\n",
    "        if token in [tokenizer.word2idx[tokenizer.cls_token], tokenizer.word2idx[tokenizer.sep_token], tokenizer.word2idx[tokenizer.pad_token]]:\n",
    "            continue\n",
    "        token_ids[index] = tokenizer.word2idx[tokenizer.mask_token]\n",
    "        masked_indices.add(index)\n",
    "    \n",
    "    labels = [-100 if i not in masked_indices else token_ids[i] for i in range(len(token_ids))]\n",
    "    return token_ids, labels\n",
    "\n",
    "\n",
    "def load_separate_and_clean_stories(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    stories = content.split('\\n\\n\\n\\n')\n",
    "\n",
    "    cleaned_stories = []\n",
    "    for story in stories:\n",
    "        cleaned_story = re.sub(r'\\n\\s*\\n', '\\n', story.strip())\n",
    "        cleaned_stories.append(cleaned_story)\n",
    "    \n",
    "    return cleaned_stories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3409336-7460-4b26-a0f2-03c9690b0764",
   "metadata": {},
   "source": [
    "## V.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e1d6409-bc2d-480b-b9bf-8e856523a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dict_,mask=False):\n",
    "        super().__init__()\n",
    "        self.dict = dict_\n",
    "        self.mask = mask\n",
    "    def __len__(self):\n",
    "        return len(self.dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        text = self.dict[str(idx)]['text']\n",
    "\n",
    "        tokens = torch.tensor(self.dict[str(idx)]['tokens'])\n",
    "\n",
    "        if self.mask!=False:\n",
    "            idxs = np.linspace(0,len(tokens)-1,len(tokens)).astype(int)\n",
    "            pos = random.choices(idxs, k=int(len(idxs)*0.15))\n",
    "            tokens[pos] = self.mask\n",
    "\n",
    "        return text,tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19421035-60b8-4739-9647-d1ccbc37e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = WordPieceTokenizer()\n",
    "tokenizer.load('wordPieceVocab.json')\n",
    "\n",
    "# Load the dataset\n",
    "dataset_txt = load_separate_and_clean_stories(\"dataset/combined_stories.txt\")\n",
    "\n",
    "dict_ = {}\n",
    "for i in range(len(dataset_txt)//20):\n",
    "    dict_[i] = {'text':dataset_txt[i],\n",
    "                'tokens':tokenizer.encode_n(dataset_txt[i])\n",
    "                 }\n",
    "\n",
    "with open(\"dataset/dataset_dict.json\", \"w\") as outfile: \n",
    "    json.dump(dict_, outfile)  # We can read this file to avoid computing again the dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08cfb7fe-7233-4d53-946e-9faf41e5d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/dataset_dict.json') as json_file:\n",
    "    dict_ = json.load(json_file)\n",
    "    \n",
    "complete_dataloader = MyDataset(dict_,tokenizer.word2idx[\"[MASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3666d63c-d386-400c-b8b1-418604d3f754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1597"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(complete_dataloader.__getitem__(0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52fc7626-ea71-4745-a77d-f151aa18a5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(complete_dataloader.__getitem__(0)[1]==4)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb7f01f-7692-4bbe-aaa3-2bd612a3fc8d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## V.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebbd024c-9690-4542-896d-1bf9644f30a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset,sentences):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.sentences = sentences\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        title = self.dataset.iloc[idx]['Title']\n",
    "        text = separate_sentences(self.dataset.iloc[idx]['cleaned_story'])\n",
    "        list_sentences = [''.join(map(str, text[i:i+self.sentences])) for i in range(0, len(text), self.sentences)]\n",
    "\n",
    "        return title,text,list_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4c6c6b5-6af7-45be-8c9d-e4a5ef80c160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>cleaned_story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Thumbelina</td>\n",
       "      <td>Once upon a time, in a world of wonder and enc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Star Money</td>\n",
       "      <td>Once upon a time, in a quaint village nestled ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The Twelve Dancing Princesses</td>\n",
       "      <td>In a kingdom where castles touched the clouds ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Elves and the Shoemaker</td>\n",
       "      <td>In a quaint village nestled at the edge of a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Fox and the Cat</td>\n",
       "      <td>Once upon a time, in a lush forest filled with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                          Title  \\\n",
       "0           0                     Thumbelina   \n",
       "1           1                 The Star Money   \n",
       "2           2  The Twelve Dancing Princesses   \n",
       "3           3    The Elves and the Shoemaker   \n",
       "4           4            The Fox and the Cat   \n",
       "\n",
       "                                       cleaned_story  \n",
       "0  Once upon a time, in a world of wonder and enc...  \n",
       "1  Once upon a time, in a quaint village nestled ...  \n",
       "2  In a kingdom where castles touched the clouds ...  \n",
       "3  In a quaint village nestled at the edge of a l...  \n",
       "4  Once upon a time, in a lush forest filled with...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('dataset/merged_stories(1).csv')\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0a03579-b228-448d-a322-272837c5a954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thumbelina\n",
      "60\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "complete_dataloader = MyDataset(dataset,6)\n",
    "print(complete_dataloader.__getitem__(0)[0])\n",
    "print(len(complete_dataloader.__getitem__(0)[1]))\n",
    "print(len(complete_dataloader.__getitem__(0)[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "04128d6c-eaf9-4357-90e6-e92c2f7a4bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My text...', 'My text.', ' My text!', ' My text?', '']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 'My text...My text. My text! My text?'\n",
    "separate_sentences(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64362273-090a-48ec-a487-3e715dafa6f8",
   "metadata": {},
   "source": [
    "## V.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3319081b-a5a2-4c7e-a0ce-37d5437509bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset,sentences):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.sentences = sentences\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        title = self.dataset.iloc[idx]['Title']\n",
    "        text = separate_sentences(self.dataset.iloc[idx]['cleaned_story'])\n",
    "        list_sentences = [''.join(map(str, text[i:i+self.sentences])) for i in range(0, len(text), self.sentences)]\n",
    "        it = random.randint(0,len(list_sentences)-2)\n",
    "        sentence = list_sentences[it]\n",
    "\n",
    "        if random.random()<0.5:\n",
    "            next_sentence = list_sentences[it+1]\n",
    "            is_next = True\n",
    "            \n",
    "        else:\n",
    "            idx2 = idx\n",
    "            while idx2 == idx:\n",
    "                idx2 = random.randint(0,len(self.dataset)-1)\n",
    "            text2 = separate_sentences(self.dataset.iloc[idx2]['cleaned_story'])\n",
    "            list_sentences2 = [''.join(map(str, text2[i:i+self.sentences])) for i in range(0, len(text2), self.sentences)]\n",
    "            it = random.randint(0,len(list_sentences2)-1)\n",
    "            next_sentence = list_sentences2[it]\n",
    "            \n",
    "            is_next = False\n",
    "        return title,sentence,next_sentence,is_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1af4ecad-31d7-4073-bd4a-ea022800a423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>cleaned_story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Thumbelina</td>\n",
       "      <td>Once upon a time, in a world of wonder and enc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Star Money</td>\n",
       "      <td>Once upon a time, in a quaint village nestled ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The Twelve Dancing Princesses</td>\n",
       "      <td>In a kingdom where castles touched the clouds ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Elves and the Shoemaker</td>\n",
       "      <td>In a quaint village nestled at the edge of a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Fox and the Cat</td>\n",
       "      <td>Once upon a time, in a lush forest filled with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                          Title  \\\n",
       "0           0                     Thumbelina   \n",
       "1           1                 The Star Money   \n",
       "2           2  The Twelve Dancing Princesses   \n",
       "3           3    The Elves and the Shoemaker   \n",
       "4           4            The Fox and the Cat   \n",
       "\n",
       "                                       cleaned_story  \n",
       "0  Once upon a time, in a world of wonder and enc...  \n",
       "1  Once upon a time, in a quaint village nestled ...  \n",
       "2  In a kingdom where castles touched the clouds ...  \n",
       "3  In a quaint village nestled at the edge of a l...  \n",
       "4  Once upon a time, in a lush forest filled with...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('/Users/francesccarandellverdaguer/fairyTaleAI/dataset/merged_stories_full.csv')\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd520e5f-3b30-4e46-adfa-9749f1ffe13a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty range for randrange() (0, 0, 0)\n",
      "Icarus and the Wax Wings: A Greek Myth\n"
     ]
    }
   ],
   "source": [
    "complete_dataloader = Custom_Dataset(dataset,2)\n",
    "for i in range(len(dataset)):\n",
    "    try:\n",
    "        complete_dataloader.__getitem__(i)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(dataset.iloc[i].Title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d44d8b-a705-4d64-8fa3-a4fe6d3e845e",
   "metadata": {},
   "source": [
    "### DATASET ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49c411b3-0f74-48c4-bd2f-92631f73b51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Story Of The Envier And The Envied. 64\n",
      "The Story Of Noor-Ed-Deen And Enees-El-Jelees. 132\n",
      "The Tale of the Envier and the Envied. 94\n",
      "The Eldest Lady's Tale. 28\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('/Users/francesccarandellverdaguer/fairyTaleAI/dataset/merged_stories_full.csv')\n",
    "dataset.head(5)\n",
    "tokenizer = WordPieceTokenizer()\n",
    "tokenizer.load('/Users/francesccarandellverdaguer/fairyTaleAI/tokenizer/wordPieceVocab.json')\n",
    "\n",
    "for idx in range(len(dataset)):\n",
    "    text = separate_sentences(dataset.iloc[idx]['cleaned_story'])\n",
    "    list_sentences = [''.join(map(str, text[i:i+2])) for i in range(0, len(text), 2)]\n",
    "    for i in range(len(list_sentences)):\n",
    "        enc = tokenizer.encode(list_sentences[i])\n",
    "\n",
    "        if len(enc)>512:\n",
    "            print(dataset.iloc[idx]['Title'],str(i))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f392ff2-bb7e-41c1-b5ba-dc09c785e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = dataset[dataset.Title==\"The Eldest Lady's Tale.\"]\n",
    "text = separate_sentences(sub.iloc[0]['cleaned_story'])\n",
    "list_sentences = [''.join(map(str, text[i:i+2])) for i in range(0, len(text), 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d71bb443-1889-4b0d-bbdd-5b292fbe414b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" a graceful shape of youth appeared in view: Saturn had stained his locks with Saturninest jet,  And spots of nut brown musk on rosy side face blew: Mars tinctured either cheek with tinct of martial red;  Sagittal shots from eyelids Sagittarius threw: Dowered him Mercury with bright mercurial wit;  Bore off the Bear what all man's evil glances grew: Amazed stood Astrophil to sight the marvel birth  When louted low the Moon at full to buss the Earth. And of a truth Allah the Most High had robed him in the raiment of perfect grace and had purfled and fringed it with a cheek all beauty and loveliness, even as the poet saith of such an one:-- By his eyelids shedding perfume and his fine slim waist I swear, By the shooting of his shafts barbed with sorcery passing rare; By the softness of his sides, and glances' lingering light,  And brow of dazzling day-tide ray and night within his hair; By his eyebrows which deny to who look upon them rest,  Now bidding now forbidding, ever dealing joy and care; By the rose that decks his cheek, and the myrtle of its moss,  By jacinths bedded in his lips and pearl his smile lays bare; By his graceful bending neck and the curving of his breast, Whose polished surface beareth those granados, lovely pair; By his heavy hips that quiver as he passeth in his pride,  Or he resteth with that waist which is slim beyond compare; By the satin of his skin, by that fine unsullied sprite;  By the beauty that containeth all things bright and debonnair; By that ever open hand; by the candour of his tongue;  By noble blood and high degree whereof he's hope and heir; Musk from him borrows muskiness she loveth to exhale  And all the airs of ambergris through him perfume the air; The sun, methinks, the broad bright sun, before my love would pale  And sans his splendour would appear a paring of his nail.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d12f16-9847-41bd-b995-920dc2ae40f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
