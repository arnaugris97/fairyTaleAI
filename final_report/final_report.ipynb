{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "071b70dd",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab5bb9d",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad79ff0a",
   "metadata": {},
   "source": [
    "[**3. Introduction**](#Introduction)\n",
    "\n",
    "- [3.1 Background & Motivation](#Background-and-motivation)\n",
    "- [3.2 Objectives & Scope](#Objectives-and-scope)\n",
    "\n",
    "[**4. Methodology**](#Methodology)\n",
    "\n",
    "- [4.1 Data Collection](#Data-Collection)\n",
    "- [4.2 Tokenizer](#Tokenizer)\n",
    "- [4.3 Model Architecture](#Model-Architecture)\n",
    "- [4.4 Training Routine](#Training-Routine)\n",
    "- [4.5 Embedding Generation](#Embedding-Generation)\n",
    "- [4.6 Vector DB](#Vector-DB)\n",
    "- [4.7 Large Language Model](#LLM)\n",
    "- [4.8 Prompting](#Prompting)\n",
    "- [4.9 UI](#UI)\n",
    "\n",
    "\n",
    "[**5. Experiments & Results**](#Experiments-&-Results)\n",
    "\n",
    "- [5.1 TensorBoard](#Tensorboard-for-metric-follow-up)\n",
    "- [5.2 Challenges](#Challenges-that-we-faced)\n",
    "- [5.3 Description of Experiments](#Description-of-the-experiments-performed)\n",
    "- [5.4 Results of the Model](#Results-of-the-model)\n",
    "\n",
    "[**6. Conclusions**](#Conclusions-&-further-exploration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353c6455",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd5dea",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Background and motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85182b4a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Natural Language Processing (NLP) has come a long way with models like BERT, which help computers understand and generate human language more effectively. This progress opens up exciting possibilities for creating engaging and contextually rich stories.\n",
    "\n",
    "The idea for this project started from a personal experience within our team. One of us had a nephew who asked for a bedtime story about \"pirates in space.\" As Catalans who value storytelling but aren't all naturally imaginative, we struggled. Attempts to use existing generative models resulted in stories that missed the context and creativity the child wanted. This frustration highlighted a gap in current story generation tools.\n",
    "\n",
    "Motivated by this experience, we set out to create a better way to generate fairy tales that truly capture the magic and context of the themes kids ask for. We decided to train a BERT model specifically for this purpose and use a Retrieval-Augmented Generation (RAG) system. This way, when someone requests a story, the system can understand the request, find similar tales, and generate a new, engaging story.\n",
    "\n",
    "Our goal is to provide a tool that can make storytelling easier and more fun, ensuring that every story is as imaginative and contextually rich as the ones we cherish from our childhood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08226f2",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Objectives and scope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b184d4c",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d04fb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The primary objective of this project is to develop an advanced story generation system that can create contextually rich and engaging fairy tales based on user input. \n",
    "\n",
    "In order to achieve this, we would divide our project into 3 different phases:\n",
    "\n",
    "![Phases of our project](initial_arquitecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f35f89",
   "metadata": {
    "hidden": true
   },
   "source": [
    "These 3 phases have been further divided into more specific tasks:\n",
    "\n",
    "**Create a Custom Tokenizer**:\n",
    "\n",
    "- Develop a tokenizer specifically designed for our dataset of fairy tales to ensure accurate text processing.\n",
    "  \n",
    "**Train a BERT Model**: \n",
    "\n",
    "- Train a BERT model using the custom tokenizer and a diverse dataset of fairy tales to enable it to understand and generate narrative content effectively.\n",
    "\n",
    "**Implement a RAG System**: \n",
    "- Develop a system that integrates a vector database for storing embeddings of fairy tales and uses these embeddings to enhance the story generation process.\n",
    "\n",
    "**Generate Rich Fairy Tales**: \n",
    "- Use the embeddings retrieved from our database as context for an LLM to generate a more rich and engaging version of a fairy tale.\n",
    "\n",
    "**Evaluate the System**: \n",
    "- Assess the performance of the system through qualitative and quantitative metrics to ensure it meets the desired objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8b975f",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Scope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51fcc0c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The scope of this project encompasses the following key areas:\n",
    "\n",
    "\n",
    "**Data Collection and Preprocessing:**\n",
    "\n",
    "- Collect a dataset of fairy tales for training the BERT model.\n",
    "- Preprocess the dataset to ensure it is suitable for training.\n",
    "\n",
    "**Tokenizer Development:**\n",
    "\n",
    "- Create a custom tokenizer tailored to the fairy tale dataset.\n",
    "\n",
    "**Model Training and Development:**\n",
    "\n",
    "- Train the BERT model on the collected dataset. We will do this by implementing:\n",
    "  - **MLM (Masked Language Model)**\n",
    "  - **NSP (Next Sentence Prediction)** tasks to fine-tune the model for narrative generation.\n",
    "\n",
    "**Embedding and Vector Database:**\n",
    "\n",
    "- Create embeddings of the fairy tales using the trained BERT model.\n",
    "- Store these embeddings in a vector database to facilitate efficient retrieval.\n",
    "\n",
    "**Retrieval-Augmented Generation System:**\n",
    "\n",
    "- Develop the retrieval mechanism using cosine similarity to find relevant story contexts.\n",
    "- Integrate the retrieval system with a language model to generate new stories based on user input.\n",
    "\n",
    "**System Evaluation:**\n",
    "\n",
    "- Conduct experiments to evaluate the retrieval accuracy and the quality of the generated stories.\n",
    "\n",
    "**User Interface:**\n",
    "\n",
    "- Design a user-friendly interface where users can input their story requests and receive generated stories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2156db34",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92272ed",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473eef11",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Data Gathering**\n",
    "\n",
    "To develop an advanced story generation system, we needed a robust and diverse dataset of fairy tales. Our data collection process involved multiple sources to ensure a comprehensive dataset. Here’s how we approached it:\n",
    "\n",
    "1. **Kaggle**: We found several relevant datasets on Kaggle, including:\n",
    "   - [Grimm’s Fairy Tales](https://www.kaggle.com/datasets/tschomacker/grimms-fairy-tales)\n",
    "   - [Grimms' Brother Fairy Tale Dataset](https://www.kaggle.com/datasets/cornellius/grimms-brother-fairy-tale-dataset)\n",
    "\n",
    "2. **Hugging Face**: We utilized datasets from Hugging Face, such as:\n",
    "   - [FairytaleQA Dataset](https://huggingface.co/datasets/WorkInTheDark/FairytaleQA)\n",
    "   - [FairyTales Dataset](https://huggingface.co/datasets/KyiThinNu/FairyTales)\n",
    "\n",
    "3. **GitHub**: We accessed the FairytaleQA dataset from GitHub:\n",
    "   - [FairytaleQAData](https://github.com/uci-soe/FairytaleQAData/tree/main)\n",
    "\n",
    "4. **Web Scraping**: To supplement the datasets, we performed web scraping on various websites dedicated to fairy tales, including:\n",
    "   - [Dream Little Star](https://dreamlittlestar.com/)\n",
    "   - [Read the Tale](https://www.readthetale.com/)\n",
    "\n",
    "We used web scraping tools and techniques to extract text data from these sources, carefully handling HTML parsing and cleaning the text to make it suitable for training.\n",
    "\n",
    "By combining datasets from these sources, we compiled a rich and diverse collection of fairy tales. This dataset was then preprocessed to remove any inconsistencies and ensure it was ready for training our BERT model.\n",
    "\n",
    "#### Dataset Analysis\n",
    "\n",
    "To better understand the characteristics of our dataset, we performed a detailed analysis, including visualizations such as histograms. Below are some of the key insights:\n",
    "\n",
    "- **Story Length Distribution**: The histogram below shows the distribution of story lengths in the dataset.\n",
    "\n",
    "![Story Length Distribution](imagen3.png)\n",
    "\n",
    "- **Vocabulary Size**: The histogram below represents the distribution of vocabulary sizes across different stories.\n",
    "\n",
    "![Vocabulary Size Distribution](imagen2.png)\n",
    "\n",
    "- **Average Sentence Length**: The histogram below shows the distribution of average sentence lengths in the dataset.\n",
    "\n",
    "![Average Sentence Length](imagen1.png)\n",
    "\n",
    "These analyses helped us to understand the dataset better and guided our preprocessing and model training steps.\n",
    "\n",
    "#### Dataset Metrics\n",
    "\n",
    "For training and validating our model, we have gathered a total of 1,183 fairy tales. Here are some key metrics of our dataset:\n",
    "\n",
    "- **Total Sentences**: 128,541 sentences\n",
    "- **Mean Sentences per Story**: 114 sentences\n",
    "- **Total Words**: 2,631,859 words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0c0847",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c7f92e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Byte-Pair Encoding (BPE) Tokenizer**\n",
    "\n",
    "Initially, our team developed a Byte-Pair Encoding (BPE) tokenizer to process the fairy tales. BPE tokenization involves merging the most frequent pairs of characters or subwords iteratively until a specified vocabulary size is reached. This method helps in efficiently encoding words and subwords, which is beneficial for the training of our BERT model.\n",
    "\n",
    "**Key steps in our BPE tokenizer implementation:**\n",
    "1. **Initialization**: Define the number of merges (iterations) for the tokenization process.\n",
    "2. **Vocabulary Creation**: Split words into characters and calculate the frequency of each pair of characters.\n",
    "3. **Pair Merging**: Iteratively merge the most frequent pairs of characters to create subwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d2287d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "for _ in range(self.num_merges):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "    best_pair = max(pairs, key=pairs.get)\n",
    "    new_symbol = ''.join(best_pair)\n",
    "    new_vocab = defaultdict(int)\n",
    "    for word in vocab:\n",
    "        new_word = word.replace(' '.join(best_pair), new_symbol)\n",
    "        new_vocab[new_word] += vocab[word]\n",
    "    vocab = new_vocab\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1577ef2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**WordPiece Tokenizer**\n",
    "\n",
    "After experimenting with the BPE tokenizer, we discovered that the WordPiece tokenizer provided better performance for our task. The WordPiece tokenizer iteratively builds the vocabulary by considering the most frequent subword pairs, similar to BPE, but with additional handling for character-level tokens.\n",
    "\n",
    "**Key advantages of the WordPiece tokenizer:**\n",
    "1. **Efficiency in handling rare words**: The WordPiece tokenizer can break down rare words into subwords more effectively, improving the model's ability to generalize.\n",
    "2. **Handling of special characters**: The tokenizer includes special tokens for punctuation, spaces, and other characters, enhancing its ability to process diverse text formats.\n",
    "\n",
    "**Key steps in our WordPiece tokenizer implementation:**\n",
    "1. **Initialization**: Define the vocabulary size and special tokens.\n",
    "2. **Word Frequency Calculation**: Count the frequency of words and characters in the text.\n",
    "3. **Vocabulary Building**: Iteratively merge the most frequent pairs to form subwords and build the vocabulary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7563d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "while len(self.vocab) < self.vocab_size:\n",
    "scores = self._compute_pair_scores(splits)\n",
    "best_pair = max(scores, key=scores.get)\n",
    "splits = self._merge_pair(*best_pair, splits)\n",
    "new_token = best_pair[0] + best_pair[1][2:] if best_pair[1].startswith(\"##\") else best_pair[0] + best_pair[1]\n",
    "self.vocab.append(new_token)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9689c4f9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Conclusion: WordPiece Tokenizer > BPE Tokenizer**\n",
    "\n",
    "We opted for the WordPiece tokenizer over the Byte-Pair Encoding (BPE) tokenizer for several key reasons:\n",
    "\n",
    "##### Key Differences\n",
    "\n",
    "1. **Handling Rare Words**:\n",
    "   - **BPE**: Less effective at handling rare words due to its frequency-based merges.\n",
    "   - **WordPiece**: Breaks down rare words into smaller, meaningful subwords, improving generalization.\n",
    "\n",
    "2. **Special Characters**:\n",
    "   - **BPE**: Lacks explicit handling of special characters.\n",
    "   - **WordPiece**: Includes special tokens for punctuation and spaces, providing accurate text representation.\n",
    "\n",
    "3. **Vocabulary Efficiency**:\n",
    "   - **BPE**: Fixed number of merges can lead to suboptimal vocabulary size.\n",
    "   - **WordPiece**: Dynamically builds a balanced and efficient vocabulary.\n",
    "\n",
    "4. **Contextual Understanding**:\n",
    "   - **BPE**: May miss contextual nuances in narrative text.\n",
    "   - **WordPiece**: Better contextual understanding due to granular word breakdown and handling of special characters.\n",
    "\n",
    "\n",
    "The WordPiece tokenizer demonstrated superior performance in handling rare words, special characters, and providing efficient, contextually aware tokenization. These advantages make it the optimal choice for processing our fairy tale dataset and generating rich, engaging stories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdc75eb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Tokenizer Output**\n",
    "\n",
    "Specifically, our final tokenizer outputs the following information:\n",
    "\n",
    "- **Token IDs**: Each token in the text is mapped to a unique integer ID from the vocabulary.\n",
    "- **Special Tokens**: These are tokens added to the text to provide additional information about the structure of the input.\n",
    "  - **[CLS]**: Added at the beginning of the text. Represents the entire input sequence and its embedding is used for classification tasks.\n",
    "  - **[SEP]**: Separates different parts of the input (e.g., question and answer, two sentences). For single sequences, it is added at the end.\n",
    "  - **[PAD]**: Used to pad sequences to the same length within a batch.\n",
    "- **Attention Mask**: Indicates which tokens should be attended to and which should be ignored (due to padding).\n",
    "- **Token Type IDs**: Identifies different segments in the input. For single sequences, all values are typically 0. For paired sequences, the first sequence might have all 0s and the second sequence all 1s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145f6e73",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc394225",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Custom BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93818327",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Visual Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b0c317",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here's the image illustrating the BERT model architecture:\n",
    "\n",
    "![Model Architecture](bert_model.png)\n",
    "\n",
    "Follow this section for specific explanations on every section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff3232",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Input of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6366646",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. **Tokenizer Output**\n",
    "\n",
    "**Components**:\n",
    "- **Input IDs**: Unique integer IDs for each token in the text.\n",
    "- **Attention Mask**: Indicates which tokens should be attended to (1) and which should be ignored (0) due to padding.\n",
    "- **Segment IDs**: Identifies different segments in the input. For single sequences, all values are typically 0. For paired sequences, the first sequence might have all 0s and the second sequence all 1s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bd3c8d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078a672e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The embedding layer converts input tokens into dense vectors.\n",
    "\n",
    "```python\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, seq_len=64, dropout=0.1):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.segment_embeddings = nn.Embedding(3, embed_size, padding_idx=0)\n",
    "        self.position_embeddings = PositionalEmbedding(d_model=embed_size, max_len=seq_len)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "       \n",
    "    def forward(self, input_ids, segment_ids):        \n",
    "        x = self.token_embeddings(input_ids) + self.position_embeddings(input_ids) + self.segment_embeddings(segment_ids)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c19a2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Key Steps:**\n",
    "\n",
    "- Convert token IDs to embeddings.\n",
    "- Add positional embeddings to encode the position of each token. (See next section)\n",
    "- Add segment embeddings to differentiate between different segments of input.\n",
    "- Apply dropout for regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f182bc",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c7ec78",
   "metadata": {
    "hidden": true
   },
   "source": [
    "3. **Positional Encoding**\n",
    "\n",
    "Adds positional information to the embeddings to ensure the model understands the order of tokens.\n",
    "\n",
    "```python\n",
    "class PositionalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_len=128):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "        for pos in range(max_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6603a6e7",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Transformer Encoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04725ab2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "4. **Transformer Encoder Blocks**\n",
    "\n",
    "Consists of multiple encoder layers that apply self-attention mechanisms and feed-forward networks.\n",
    "\n",
    "```python\n",
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model=768, heads=12, feed_forward_hidden=768 * 4, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.layernorm = torch.nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadedAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, embeddings, mask):\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aa42a1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Key Steps:**\n",
    "\n",
    " - **Self-Attention:** Allows the model to focus on different parts of the input sequence.\n",
    " - **Feed-Forward Network:** Applies a fully connected feed-forward network to each token.\n",
    " - **Layer Normalization and Dropout:** Normalizes the output and applies dropout for regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26962019",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Training Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9028d5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Explanation of our training tasks:**\n",
    "- For our **MLM task**, our goal is to predict some tokens that we have previously masked\n",
    "- For our **NSP task**, we will try to predict whether the second sentence of our input is the following sentence of our dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e833356",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### MLM: Masked Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0612d1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "class MaskedLanguageModel(nn.Module):\n",
    "    def __init__(self, hidden, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.linear(input)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255ff56a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Key Steps:**\n",
    "- Use a linear followed by a softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bff412",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### NSP: Next sentence Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055607d5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "class NextSentencePrediction(nn.Module):\n",
    "    def __init__(self, hidden):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = input[:, 0]\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf263b6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Key Steps:**\n",
    "- Use only the first token ([CLS]) to predict if the next sentence follows.\n",
    "- Apply a linear layer followed by a softmax for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51947964",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Transfer Learning with DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed35ed",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As a second approach for building a model that generates coherent embeddings, we opted for fine-tuning a pretrained DistilBERT model. DistilBERT is a transformer model based on the already described BERT architecture, but containing 40% less parameters. The DistilBERT base model training uses knowledge distillation, which compresses a larger model known as teacher (in this case BERT) into a smaller model called student (in this case DistilBERT)[1](https://arxiv.org/abs/1910.01108).\n",
    "\n",
    "In the context of our project, the pretrained DistilBERT model was imported from [huggingface](https://huggingface.co/distilbert/distilbert-base-uncased). This model was originally trained using the BookCorpus and English Wikipedia datasets to perform the same objective tasks as we described previously: masked language modelling and next sentence prediction. The DistilBERT model's follows a practically identical architecture as the one previously described for the developed custom BERT model. In this case, however, the model contains 6 transformer encoder blocks.\n",
    "\n",
    "To build and train our DistilBERT model, we performed transfer learning from a trained DistilBERT model in order to start our training routine with a model with pretrained weights in all of its blocks (embedding layer, transformer encoder blocks, MLM head block and NSP head block). We then kept frozen all model weights but the ones belonging to the last transformer encoder, MLM head and NSP head blocks. This way, the training process is simplified since the number of trainable weights is drastically reduced, while keeping unmodified the weights transfered from a more extensive pretraining process. \n",
    "Last but not least, in order to use the pretrained model, the specific Distilbert pretrained tokenizer (with a vocabulary size of 30522 tokens) was used to process and tokenize our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af35ebf",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "\n",
    "class BERT_TL(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Language Model - Fine-tuning DistilBERT\n",
    "    Next Sentence Prediction Model + Masked Language Model\n",
    "    Separated to be able to do inference to the main model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        :param bert: BERT model which should be trained\n",
    "        :param vocab_size: total vocab size for masked_lm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        model_MLM = DistilBertForMaskedLM.from_pretrained(\"distilbert-base-uncased\")\n",
    "        model_NSP = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "        self.bert = model_MLM.distilbert\n",
    "        for param in self.bert.parameters(): # we just keep unfrozen the last encoder block\n",
    "            param.requires_grad = False\n",
    "        for param in self.bert.transformer.layer[-1].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.next_sentence = nn.Sequential(model_NSP.pre_classifier,model_NSP.classifier) \n",
    "        self.mask_lm = nn.Sequential(model_MLM.vocab_transform,model_MLM.vocab_layer_norm, model_MLM.vocab_projector,nn.LogSoftmax(dim=-1))\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3105c7c0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Training Routine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de675df",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Dataset split for training, validating and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52598091",
   "metadata": {
    "hidden": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The built dataset was split into three subsets in order to generate a train set, which will be used to compute the loss during the training step and update the model's weights through backpropagation; a validation set, which will be used to evaluate the model during training and adjust different hyperparameters correctly; and a test set, which is used to evaluate the model's generalization capabilities after performing all training steps. These splits were performed at a \"text\" level to avoid introducing sentences coming from the same text in different subsets, which in our case is key to obtain independent splits for performing each of the aforementioned tasks.\n",
    "\n",
    "We performed a train-validation-test split of 80%-10%-10%. The main statistics of each subset are described in the table below:\n",
    "| Subset |Number of texts | Number of sentences |\n",
    "|----------|----------|----------|\n",
    "| **Train set**   | 894   | 107326|\n",
    "| **Validation set**   | 112  |12992 |\n",
    "| **Test set**   | 112   |10938 |\n",
    "\n",
    "To ensure that each subset was representative of the whole dataset, we also compared how distributions of different parameters of each subset were statistically similar using ---INTRODUCE TEST, FALTA POSAR TAULA AMB EL TEST--- with respect to the complete dataset. The results of these statistical tests can be found in the following figure:\n",
    "\n",
    "![Subset splits](split.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e190a9c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Hyperparameters and training methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adee1a9d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Learning rate scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d474b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "During the developed training routine for our model we decided to use a learning rate scheduler, which is a mechanism used for adjusting the learning rate value during the training process. The learning rate determines the size of the steps the optimization algorithm takes when updating the model's weights. Proper adjustment of the learning rate is crucial because it can significantly affect the training dynamics and the performance of the model. There are several functions that can be used for updating the learning rate (such as step decay, exponential decay, reduce on plateau or cosine annealing, to name a few). \n",
    "\n",
    "In our case, since we wanted to start by using a higher learning rate value for an initial adjustment of the model weights and then find the optimal weights using a learning rate with a lower value, we decided to initially increase by a linear factor the learning rate during a fixed number of warm up steps, and then use an exponential decay scheduler until reaching the final learning rate fixed value. Thus, both the number of warm up steps and the final learning rate value were hyperparameters that needed to be adjusted. In the figure below it is shown an example of the implemented learning rate scheduler.\n",
    "\n",
    "<img src=\"learning_Rate_scheduler.png\" alt=\"drawing\" width=\"600\" height=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0914ff8d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9990bee7",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf3a0c7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As it was previously explained, training our BERT model implies performing two independent tasks: predicting the correct token for each masked one in the MLM head; and predicting if the second sentence of our input is the following sentence of the same text in the NSP head. In essence, both of them are classification tasks. As such, we will need to use an individual loss function that is able to compare the predicted value of each head with its true value, and then add both values to compute the total loss for a given batch of samples: $$ Loss = Loss_{MLM} + Loss_{NSP} $$\n",
    "\n",
    "For training the custom BERT model, we decided to use negative log-likelihood loss for evaluating both MLM and NSP tasks. This is why we use a logarithmic softmax as our final activation function for both heads. On the other hand, for fine-tuning the pretrained DistilBERT we used two different loss functions: the MLM task was evaluated using the log-likelihood loss, while the NSP task was assessed using the cross-entropy loss. This change was motivated since the latter loss function was used for evaluating the NSP task in the original training of DistilBERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e97a2d6",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Batch size and gradient accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ec4cf2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When building each batch during each training step, either train or validation batch, a single masked sentence pair for each text is introduced (until fulfilling the batch size). This ensures that all items in a batch originally come from different texts. The batch size was set as a hyperparameter so as to handle local resources constraints and have a balance between training speed, stability and convergence.\n",
    "\n",
    "On the other hand, we introduced gradient accumulation during training. Gradient accumulation is used to effectively increase the batch size without requiring more resources than available. When using this method, the gradient is accumulated over multiple mini-batches before updating the weights through backpropagation, simulating a larger batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dc65af",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4995073",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The optimizer can be described as the method used to adjust the weights of the neural network in order to minimize the loss function during training. In our case, we decided to use the Adam algorithm with a fixed value for the betas coefficients of (0.9, 0.999) and weight decay of 0.01 for all experiments, while the learning rate was adjusted as a hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa322fd",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Early stopper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752581b1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In order to avoid overfitting and diminishing the validation set performance during training, we decided to implement an early stopper function as a regularization method. In essence, during each epoch the early stopper checks if the validation loss is higher than the one in the best performing epoch. If this condition is fulfilled, the best performing epoch is updated and the training routine continues as scheduled. On the other hand, if the current epoch's validation performance is lower than the best observed so far, the training is stopped if the performance has not improve for a specific number of epochs (patience). The patience parameter was set to 10 epochs. \n",
    "\n",
    "```python\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            print('*')\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9411c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Embedding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b2bd0d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Custom BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88274163",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Inference and Embedding Generation**\n",
    "\n",
    "During inference, we use the BERT model **without** the **Next Sentence Prediction (NSP)** and **Masked Language Model (MLM)** tasks. Instead, we focus on generating meaningful embeddings for our fairy tale dataset. These embeddings capture the contextual information of the input text, which is crucial for our RAG system.\n",
    "\n",
    "**Embedding Generation:**\n",
    "- The BERT model's embedding layer and encoder layers are used to transform input text into dense vector representations.\n",
    "- These embeddings encapsulate the semantic meaning and context of the input text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1fb91",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Visual Summary**\n",
    "Here's the image illustrating the BERT model architecture:\n",
    "\n",
    "![Model Architecture](bert_inf.png)\n",
    "\n",
    "Follow this section for specific explanations on every step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fd92f9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len=512, d_model=768, n_layers=12, heads=12, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.heads = heads\n",
    "        self.feed_forward_hidden = d_model * 4\n",
    "        self.embedding = EmbeddingLayer(vocab_size=vocab_size, embed_size=d_model, seq_len=seq_len, dropout=dropout)\n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, heads, d_model * 4, dropout) for _ in range(n_layers)])\n",
    "    \n",
    "    def forward(self, x, segment_info):\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "        x = self.embedding(x, segment_info)\n",
    "        for encoder in self.encoder_blocks:\n",
    "            x = encoder.forward(x, mask)\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44317b24",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Transfer Learning with DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc79df0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87b2828",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Our project involved creating an embedding database for storing and querying sentence embeddings generated by a BERT model. We experimented with two different solutions: **Chroma DB** and **Zilliz**. Below are the key steps involved in each implementation and the reasoning behind our final choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b12ee4",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Chroma DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad3826",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Key Steps:**\n",
    "1. **Initialization:**\n",
    "   - Set up paths for the model, tokenizer, and CSV file containing the sentences.\n",
    "   - Initialize the ChromaDB client with a persistent storage path.\n",
    "\n",
    "2. **Model and Tokenizer Loading:**\n",
    "   - Load the BERT model and tokenizer using the provided paths.\n",
    "   - Set the model to evaluation mode.\n",
    "\n",
    "3. **Data Loading:**\n",
    "   - Read sentences and titles from the CSV file into a Pandas DataFrame.\n",
    "\n",
    "4. **Embedding Generation and Storage:**\n",
    "   - Use a custom dataset class (`Custom_Dataset_DB`) and DataLoader to handle batching.\n",
    "   - Generate embeddings for each batch of sentences.\n",
    "   - Store the embeddings, original sentences, and metadata (titles) in ChromaDB.\n",
    "\n",
    "5. **Querying:**\n",
    "   - Implement a method to query the database using a given sentence embedding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa8d559",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "class ChromaEmbeddingProcessor:\n",
    "    def __init__(self, model_path, tokenizer_path, csv_file, storage_path):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer_path = tokenizer_path\n",
    "        self.csv_file = csv_file\n",
    "        self.collection_name = \"BERT_embeddings\"\n",
    "        self.chromadb_client = self.initialize_chromadb_client(storage_path)\n",
    "        self.model, self.tokenizer = self.load_bert_model()\n",
    "        self.model.eval()\n",
    "\n",
    "    def initialize_chromadb_client(self, storage_path):\n",
    "        return chromadb.PersistentClient(storage_path)\n",
    "\n",
    "    def load_bert_model(self):\n",
    "        return load_model(self.model_path, self.tokenizer_path)\n",
    "\n",
    "    def load_sentences_from_csv(self):\n",
    "        return pd.read_csv(self.csv_file)\n",
    "\n",
    "    def process_and_store_embeddings(self):\n",
    "        dataset = Custom_Dataset_DB(self.load_sentences_from_csv(), self.tokenizer, 512)\n",
    "        dataloader = DataLoader(dataset, batch_size=64, shuffle=False, drop_last=False)\n",
    "        for step, data in enumerate(dataloader):\n",
    "            sentence_embeddings = generate_embeddings(data, self.model)\n",
    "            embeddings = [emb.detach().numpy().flatten().tolist() for emb in sentence_embeddings]\n",
    "            documents = [data['sentence'][i] for i in range(len(sentence_embeddings))]\n",
    "            titles = [{\"title\": data['title'][i]} for i in range(len(sentence_embeddings))]\n",
    "            ids = [f\"id{step*64+i}\" for i in range(len(sentence_embeddings))]\n",
    "            collection = self.chromadb_client.get_collection(name=self.collection_name)\n",
    "            collection.add(documents=documents, embeddings=embeddings, metadatas=titles, ids=ids)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2c330e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Zilliz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64e4661",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "**Key Steps:**\n",
    "\n",
    "**Initialization:**\n",
    "- Set up paths for the model, tokenizer, and CSV file containing the sentences.\n",
    "- Connect to the Milvus server hosted on Zilliz.\n",
    "\n",
    "**Model and Tokenizer Loading:**\n",
    "- Load the fine-tuned BERT model and tokenizer.\n",
    "- Set the model to evaluation mode.\n",
    "\n",
    "**Data Loading:**\n",
    "- Read sentences and titles from the CSV file into a Pandas DataFrame.\n",
    "\n",
    "**Embedding Generation and Storage:**\n",
    "- Use a custom dataset class (`Custom_Inference_TL_Dataset_DB`) and DataLoader to handle batching.\n",
    "- Generate embeddings for each batch of sentences.\n",
    "- Store the embeddings, original sentences, and metadata (titles) in Zilliz.\n",
    "\n",
    "**Querying:**\n",
    "- Implement a method to query the database using a given sentence embedding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb8bc3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "class MilvusEmbeddingProcessorTL:\n",
    "    def __init__(self, model_path, csv_file):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.csv_file = csv_file\n",
    "        self.collection_name = \"BERT_embeddings_trained\"\n",
    "        self.model = self.load_bert_model()\n",
    "        self.model.eval()\n",
    "        self.client = MilvusClient(uri=\"...\", token=\"...\")\n",
    "\n",
    "    def load_bert_model(self):\n",
    "        return load_model_TL(self.model_path, self.tokenizer)\n",
    "\n",
    "    def load_sentences_from_csv(self):\n",
    "        return pd.read_csv(self.csv_file)\n",
    "\n",
    "    def process_and_store_embeddings(self):\n",
    "        dataset = Custom_Inference_TL_Dataset_DB(self.load_sentences_from_csv(), self.tokenizer, 512)\n",
    "        dataloader = DataLoader(dataset, batch_size=64, shuffle=False, drop_last=False)\n",
    "        id_counter = 0\n",
    "        for step, data in enumerate(dataloader):\n",
    "            sentence_embeddings = generate_TL_embeddings(data, self.model)\n",
    "            embeddings = [emb.tolist() for emb in sentence_embeddings]\n",
    "            texts = [data['sentence'][i] for i in range(len(sentence_embeddings))]\n",
    "            titles = [data['title'][i] for i in range(len(sentence_embeddings))]\n",
    "            entities = [{\"id\": id_counter + i, \"embedding\": emb, \"text\": txt, \"title\": title} for i, (emb, txt, title) in enumerate(zip(embeddings, texts, titles))]\n",
    "            id_counter += len(entities)\n",
    "            self.client.insert(collection_name=self.collection_name, data=entities)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d059d9",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Why Zilliz over Chroma DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5fd2dc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Reason for Choosing Zilliz over Chroma**\n",
    "\n",
    "- **Scalability:** The final vector database created with Chroma DB weighed over 26 GB, making it impractical for our needs. Zilliz offered a more scalable solution that better handled the large volume of data.\n",
    "\n",
    "- **Performance:** Zilliz provided faster insertion and querying capabilities, essential for handling our large dataset efficiently.\n",
    "\n",
    "- **Ease of Use:** The integration with Milvus and the provided Python client made Zilliz straightforward to implement and maintain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ed38e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819e6eef",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A Large Language Model (LLMs) is a computational model made of neural networks trained on a huge data corpus to perform like humans in some tasks. In our specific case, we want to focus on text generation tasks and how the model  can create good quality fairy tales. To achieve that, the LLMs use transformers inside the neural network to perform these natural language procession (NLP) tasks.\n",
    "\n",
    "We want to find one of the best options to generate stories and text. One of the best LLMs that we can use today for storytelling is Claude 3.5 sonnet. This model is not open source, but we have an API key to perform inferences. In order to try other LLMs, we have integrated Llama 3 and GPT-3.5 turbo instruct.\n",
    "\n",
    "Llama 3 is run locally with Ollama. We wanted to run one of them locally to see how is performing compared to the other cloud based models. Another point is to see which is the difference between a 7B parameters like the Llama 3 run locally and Claude 3.5 sonnet or GPT-3.5 Turbo instruct run in their servers and with a very huge number of parameters.\n",
    "\n",
    "For this reason, in our project, the user has the option to perform the inference with three LLMs. Can run with Llama 3 using Ollama and Langchain, or use Claude 3.5 sonnet, or use GPT-3.5 Turbo. The first one is slower than the others because it is executed locally. Then the size matter in this case, because the output of the Llama 3 is not the same as the output of the other models. The cloud base models are larger than Llama 3 and the quality is better.\n",
    "\n",
    "Now we will show you which is the output for the same input for each of the models:\n",
    "\n",
    "**The input prompt:**\n",
    "\n",
    "This is a prompt for a RAG system. I need you to create a fairy tale in Catalan following the user prompt and the context. The user prompt is: Once upon a time there was a little red riding hood who lived in a small village.\n",
    "\n",
    "And this is the context: I once went twelve miles in a coop, and it was not pleasant travelling at all., THERE was once upon a time a couple of folks who had a son called Halvor., There was a loud yell of pain, and then the ogre dashed in front of the warrior., Once upon a time there was a fisherman who had a wife and three children., This place was said to be haunted by a cannibal goblin who took the form of an old woman., Those sounds came from a little street that led to a small village along the shore., About a mile farther on the road, there is another herd for you., Once upon a time two young men living in a small village fell in love with the same girl., There was no one in the house except an old Lapland woman who was cooking fish over an oil-lamp., Then he went on and came to a wide river over which he must go..\n",
    "\n",
    "So the output should be in Catalan.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**This is the output with local Llama 3:**\n",
    "\n",
    "Quan era petit, hi havia una xiqueta vermella que vivia en un petit poble. La xiqueta es deia Xisca i era filla d'un bon camperol que tenia molts animals al seu camp. Ella era molt alegre i amorosa, i sempre estava disposada a ajudar els seus veïns.\n",
    "\n",
    "Però un dia, mentre anava a visitar la seva àvia a les muntanyes, Xisca es va trobar amb una situació molt desagradable. Va sortir del poble i va començar a caminar per una carretera llarga i estreta que duia fins al costat d'un gran llac. Hi havia un home d'una força impressionant, anomenat Torsten, que havia construït una caserna al centre del poble.\n",
    "\n",
    "Quan Xisca es va acostar a la caserna, l'esguardava amb curiositat i inquietud. De sobte, sentí un crit agònic i llavors l'home d'una força impressionant, Torsten, va sortir de la caserna i va començar a córrer cap al poble.\n",
    "\n",
    "La xiqueta Xisca es va trobar amb una dona vella que tenia una barba llarga i unes ulls estranys. La dona li digué: \"No t'acostis, xiqueta. Aquest home és un ogre i és molt perillós\". Però Xisca no la va escoltar i es va acostar a ell.\n",
    "\n",
    "Llavors, l'ogre la va agafar i se la va emportar cap al seu castell. La xiqueta Xisca era molt espantada i no sabia què fer. Però llavors sentí una veu que li deien: \"Xiqueta, no t'afegeixis. Aquest home és un ogre i és molt perillós\".\n",
    "\n",
    "La xiqueta es va despertar i es va trobar al seu propi llit a casa seva. Era tot un somni! Però Xisca sabia que l'aventura no havia acabat encara. Es preparà per anar a visitar la seva àvia a les muntanyes, ja que havia promès de fer-ho.\n",
    "\n",
    "Però abans d'anar-se'n, va sentir una veu que li deien: \"Xiqueta, no t'afegeixis. Aquest home és un ogre i és molt perillós\". I llavors es va adonar que l'aventura havia començat realment!\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "**This is the output with GPT-3.5 Turbo instruct:**\n",
    "\n",
    "Hi ha una vegada en un petit poble una nena amb una caputxeta vermella que vivia amb la seva àvia. Un dia, la seva àvia li va demanar que portés una cistella de menjar a la seva mare, que vivia a dotze milles de distància. La nena va acceptar, tot i que no li agradava gaire viatjar en una carrossa.\n",
    "\n",
    "Mentre viatjava, la nena va passar per un bosc on va trobar una parella de gent gran que tenia un fill anomenat Halvor. Ells li van explicar que havien sentit un crit fort de dolor i que un ogre havia aparegut davant d'un guerrer. La nena va continuar el seu camí, pensant en aquesta història.\n",
    "\n",
    "Finalment, va arribar a la casa de la seva mare, que era una pescadora i tenia tres fills. La nena va ajudar la seva mare a preparar el peix per al sopar i li va explicar la història de l'ogre que havia sent\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**This is the output with Claude 3.5 Sonnet:**\n",
    "\n",
    "Hi havia una vegada una Caputxeta Vermella que vivia en un petit poble vora el mar. Un dia, la seva mare li va demanar que portés un cistell de menjar a la seva àvia malalta, que vivia a l'altra banda del bosc.\n",
    "\n",
    "Mentre caminava pel camí, la Caputxeta va sentir uns sorolls estranys que provenien d'un carreró que conduïa a un altre llogaret. Intrigada, va decidir investigar-ho.\n",
    "\n",
    "En arribar-hi, va trobar una vella lapona que cuinava peix sobre una llàntia d'oli. La vella li va advertir sobre un goblin caníbal que rondava per aquells indrets, disfressat de dona gran.\n",
    "\n",
    "Espantada, la Caputxeta va reprendre el seu camí, però aviat es va trobar amb un gran riu que havia de creuar. Mentre buscava la manera de passar, va veure un pescador amb la seva dona i tres fills que lluitaven contra un ogre ferotge.\n",
    "\n",
    "La Caputxeta, valenta, va ajudar la família a espantar l'ogre, que va fugir cridant de dolor. Agraït, el pescador li va indicar un camí segur per travessar el riu i arribar a casa de la seva àvia.\n",
    "\n",
    "Finalment, la Caputxeta va aconseguir arribar a casa de l'àvia, on va descobrir que la suposada \"àvia\" era en realitat el goblin caníbal disfressat. Amb enginy i l'ajuda d'un jove anomenat Halvor, que passava per allà, van aconseguir derrotar el goblin i salvar la veritable àvia.\n",
    "\n",
    "I així, la Caputxeta va tornar al seu poble, havent viscut una gran aventura i havent fet nous amics. Des d'aquell dia, sempre explicava que viatjar dotze milles en un galliner no era gens agradable, però que les aventures que havia viscut havien valgut la pena.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "As we can see, if we don't specify a length, the output of GPT-3.5 Turbo is shorter than the others. But the main difference between them is the quality of the grammar and the names that they use. From our point of view, Claude 3.5 has the best Catalan and the coherence is very good. For GPT-3.5 Turbo is also good, the Catalan, but not as good as Claude. Both, Claude and GPT-3.5 Turbo have a good output quality but are larger than Llama 3 and are executed on the cloud. Llama 3 has a poor quality of Catalan, but the coherence is good.\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "1. **Setup and Initialization:**\n",
    "   - Initialize the necessary components including the pre-trained embedding model and the LLMs.\n",
    "\n",
    "2. **Processing and Generating Text:**\n",
    "   - Process the user prompt to retrieve relevant context using the embedding database.\n",
    "   - Construct a prompt template for the LLMs.\n",
    "   - Generate the fairy tale using the selected LLM.\n",
    "\n",
    "3. **Displaying Results:**\n",
    "   - Display the generated fairy tale along with detailed information.\n",
    "\n",
    "\n",
    "**Setup and Initialization:**\n",
    "\n",
    "```python\n",
    "model_path = 'Checkpoints/...'\n",
    "tokenizer_path = 'tokenizer/...'\n",
    "\n",
    "processor = MilvusEmbeddingProcessorTL(model_path, '')\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "llmGPT = OpenAI(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=ANTROPHIC_API_KEY,\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb345fe4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Example of the implementation:**\n",
    "    \n",
    "```python  \n",
    " # Using the selected model for inference\n",
    "if model_choice == \"Llama 3\":\n",
    "    llm_result = llm.invoke(prompt_template)\n",
    "elif model_choice == \"ChatGPT\":\n",
    "    llm_result = llmGPT.invoke(prompt_template)\n",
    "else :\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt_template}\n",
    "        ]\n",
    "    )\n",
    "    llm_result = message.content[0].text\n",
    "            \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476d5044",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aec867",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the context of prompting, we want to apply different techniques to find out which is the best one for our specific RAG system. The structure of the prompt can improve significantly the LLMs output. For this reason, we want to try different techniques. In a RAG system, prompts are very important to ensure a good quality in the output. The points that we can focus on the prompt are stating the task that we want to do, providing context and specifying the desired format or language of the response.\n",
    "\n",
    "Now we will show you different templates of prompt focusing on different points. The output for these prompt will be shown in the experiments section.\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "**Template 1: Detailed Instruction with Format Emphasis**\n",
    "\n",
    "You are a creative storyteller. Using the following user prompt and context, craft a fairy tale in Catalan. Ensure the story is engaging and follows a clear structure with a beginning, middle, and end. User prompt: {userPrompt}. Context: {context}. Please write the story in Catalan.\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "**Template 2: Emphasizing Creativity and Contextual Relevance**\n",
    "\n",
    "As a master storyteller, your task is to weave an enchanting fairy tale in Catalan. Use the provided user prompt and context to inspire your story. The user prompt is: {userPrompt}. Here is the context for reference: {context}. Let your imagination run wild, and remember to write the tale in Catalan.\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "**Template 3: Focus on Emotional Engagement and Language**\n",
    "\n",
    "Imagine you are telling a magical fairy tale in Catalan, inspired by the user prompt and context given. The user prompt is: {userPrompt}. The context to guide your story is: {context}. Create a captivating narrative that evokes wonder and delight, and ensure it's written in Catalan.\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "**Template 4: Concise and Direct with Contextual Clarity**\n",
    "\n",
    "Create a fairy tale in Catalan based on the following details. User prompt: {userPrompt}. Context: {context}. Ensure the story is imaginative and written in Catalan.\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "**Template 5: Encouraging Detailed and Vivid Storytelling**\n",
    "\n",
    "You are an expert in crafting vivid fairy tales. Using the user prompt and context provided, write a detailed and imaginative fairy tale in Catalan. User prompt: {userPrompt}. Context: {context}. Make sure the story is full of rich descriptions and engaging elements, and is written in Catalan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038053ba",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea908b6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "All RAG systems should have a UI to be used by the users and fulfil their use cases. To achieve this, we have created a UI with Streamlit that can handle two flows.\n",
    "First flow is to fulfil the user use case. Where the user types a prompt and selects the preferred LLM and the system generates a fairy tale. The other flow has the same information but shows the prompt tokenization, which context the system retrieve, which is the system prompt and the final output of the LLM.\n",
    "\n",
    "![Initial state UI](initial_state_ui.png)\n",
    "\n",
    "\n",
    "![UI first flow](UI_first_flow.png)\n",
    "\n",
    "\n",
    "![UI second flow](UI_second_flow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e31d2",
   "metadata": {},
   "source": [
    "# Experiments & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76f2cca",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Tensorboard for metric follow-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cd2520",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To monitor the training and validation process, we integrated TensorBoard into both our custom BERT model and the transfer learning model. TensorBoard provides a visual interface to track metrics like loss and accuracy, helping us to better understand the model's performance over time.\n",
    "\n",
    "**TensorBoardLogger Class**\n",
    "\n",
    "We created a `TensorBoardLogger` class to handle the logging. This class uses PyTorch's `SummaryWriter` to write logs that TensorBoard can read and display.\n",
    "\n",
    "**TensorBoardLogger Implementation:**\n",
    "```python\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class TensorBoardLogger:\n",
    "    def __init__(self, base_log_dir='...'):\n",
    "        log_dir = self._get_log_dir(base_log_dir)\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "\n",
    "    def _get_log_dir(self, base_log_dir):\n",
    "        current_time = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "        return os.path.join(base_log_dir, current_time)\n",
    "\n",
    "    def log_training_loss(self, loss, epoch, step, total_steps):\n",
    "        self.writer.add_scalar('Training Loss', loss, epoch * total_steps + step)\n",
    "\n",
    "    def log_average_training_loss(self, avg_loss, epoch):\n",
    "        self.writer.add_scalar('Average Training Loss', avg_loss, epoch)\n",
    "\n",
    "    def log_validation_loss(self, avg_loss, epoch):\n",
    "        self.writer.add_scalar('Validation Loss', avg_loss, epoch)\n",
    "    \n",
    "    def log_validation_accuracy_nsp(self, accuracy, epoch):\n",
    "        self.writer.add_scalar('Validation Accuracy NSP', accuracy, epoch)\n",
    "    \n",
    "    def log_validation_accuracy_mlm(self, accuracy, epoch):\n",
    "        self.writer.add_scalar('Validation Accuracy MLM top 1', accuracy, epoch)\n",
    "\n",
    "    def log_validation_accuracy_mlm_top5(self, accuracy, epoch):\n",
    "        self.writer.add_scalar('Validation Accuracy MLM top 5', accuracy, epoch)\n",
    "\n",
    "    def log_validation_accuracy_mlm_top10(self, accuracy, epoch):\n",
    "        self.writer.add_scalar('Validation Accuracy MLM top 10', accuracy, epoch)\n",
    "\n",
    "    def close(self):\n",
    "        self.writer.close()\n",
    "        \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a4b74",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Challenges that we faced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd1179",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41310be4",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Token Length adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9028e680",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In our first iterations of training, we would have the issue where the combined loss of our training tasks would return NaN at what it seemed to be completely random iterations. \n",
    "\n",
    "Sometimes, it would be in our second epoch. Sometimes later. This gave us a hard time spotting the specific place where this error was coming from.\n",
    "\n",
    "**Solution**\n",
    "\n",
    "At that point, the maximum length of our tokens was 512. Most of the sentences that we were processing had a lower token count than that. So, in most of those cases, we would then add [PAD] (padding) tokens to make up for the difference up to 512 tokens. \n",
    "\n",
    "However, we eventually realized, through extensive search of the specific embeddings that were crashing, that whenever the combination of embedding of the 2 sentences was large enough (exactly 512), no [PAD] would be added. In those cases, our attention Mask would fail to understand which sections of the sentence required attention and which not. \n",
    "\n",
    "We solved it by making sure that, whenever a sentence was combined with another sentece that had exactly 512 tokens, we would then just look for another sentece that did not cause this issue. Combinations that were longer than 512 were discarded from the start."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2326390b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Labeling of Masked Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54126885",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After debugging these issues, our training finally started without errors. However, we started to relize that our model was failing to learn properly:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b7356f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Tokenizer modification**\n",
    "\n",
    "On of our initial approaches was to switch our Tokenizer for the original BERT tokenizer, to make sure that our custom tokenizer wasnt the problem. However, after making the change, we realized that the training process would still stagnate eventually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d244682",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"loss_0.png\" alt=\"Loss of our first iterations of training\" style=\"width:50%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27953ce1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"Acc_0.png\" alt=\"Accuracy of our first iterations of training\" style=\"width:50%;\">\n",
    "\n",
    "- **Blue & Grey** lines are the loss using the BERT tokenizer\n",
    "- **Green** line used our custom tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a65243",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Some hyperparameter modifications**\n",
    "\n",
    "We then tried to increase the size of the model as well as adding some quality of life modifications for our model.\n",
    "\n",
    "*We applied the following modifications:\n",
    "- Batch Size increase\n",
    "- Modification of segment IDs \n",
    "- More epochs\n",
    "- Adding Warmup & Scheduler\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24f7000",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"loss_1.png\" alt=\"Loss\" style=\"width:50%;\">\n",
    "\n",
    "<img src=\"Acc_1.png\" alt=\"Accuracy\" style=\"width:50%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c45d03",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Even though it looked much better than before, our Loss continued to stagnate, and the accuracy of our MLM task was very low, clearly indicating that the model was not yet learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5011634d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Solution**\n",
    "\n",
    "we finally realized that there was a problem in the masking inside of the encoder\n",
    "\n",
    "**Padding Mask:** In the encoder, there was a mask used for padding that was incorrectly set up. The padding mask should typically ignore the padding tokens (usually set to **False** for padding and **True** for actual content), but it was the other way around (padding was marked as **True** and content as **False**).\n",
    "\n",
    "We then, set up an experiment with batch size 1, in order to see if the model would overfit. And it did:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd554387",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"Acc_2.png\" alt=\"Accuracy\" style=\"width:30%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dee278b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**New Metric**\n",
    "- At this point, we also decided to add a top 5 token accuracy\n",
    "\n",
    "After performing another complete training, we saw that our training was already not stagnating as much"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b6df60",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"Acc_3.png\" alt=\"Accuracy\" style=\"width:30%;\">\n",
    "<img src=\"Acc_4.png\" alt=\"Accuracy\" style=\"width:30%;\">\n",
    "<img src=\"loss_2.png\" alt=\"Accuracy\" style=\"width:30%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164c6262",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### NSP Continues to be an issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e2799b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "At this point, our MLM task was learning preety well, and it was all about tuning the hyperparamenters.\n",
    "However we observed that our NSP (Next sentence predict) task was still completely random.\n",
    "\n",
    "- This indicated that even though our model was learning the vocabulary of our tokenizer, it would not understand the semantic meaning of the sentences good enough to be able to tell if a sentence was close to another sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36013928",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Weighted Loss**\n",
    "\n",
    "One of the first things we tried was to give different weigths to the losses of the NSP task and the MLM task in order to force the model to have a better performance in the NSP task.\n",
    "\n",
    "This did not really work. \n",
    "\n",
    "**Learing Rate and other hyperparameters**\n",
    "\n",
    " - We tried lowering the LR even though our scheduler would do the same thing\n",
    " - Increased hidden size of our forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd486e08",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Dataset Issues**\n",
    "\n",
    "One of our fears from the start was that our dataset would not be good enough for this complex task.\n",
    "At the end of the day, we finally realized that this was the case.\n",
    "\n",
    "Roughly 1.200 fairy tales is not enough for a model to learn the ropes of a language from scratch.\n",
    "\n",
    "At this point we were very disappointed, but we had to improvise a solution to make the model work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f1fb67",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Transfer Learning**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c56bcd",
   "metadata": {},
   "source": [
    "### Description of the experiments performed\n",
    "\n",
    "- Validate the custom model performing overfitting\n",
    "\n",
    "    ![OverfittingI](acc_2.png)\n",
    "\n",
    "- Custom model with custom tokenizer, 100 batch size, 6 accomulation steps vs 64 batch size, 3 accomulation steps\n",
    "\n",
    "    <img src=\"experiment1.png\" width=\"50%\"/>\n",
    "    <img src=\"experiment2.png\" width=\"50%\"/>\n",
    "    <img src=\"experiment3.png\" width=\"50%\"/>\n",
    "\n",
    "\n",
    "Conclusion: Same accuracies and is not learning for NSP task\n",
    "\n",
    "- Use BERT tokenizer with custom model and only masl 7% of the tokens vs mask 15% of the tokens\n",
    "    <img src=\"experiment4.png\" width=\"50%\"/>\n",
    "    <img src=\"experiment5.png\" width=\"50%\"/>\n",
    "    <img src=\"experiment6.png\" width=\"50%\"/>\n",
    "    \n",
    "- Transfer learning with 20 epochs\n",
    "\n",
    "- Transfer learning with 100 epochs (early stopper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83db2e51",
   "metadata": {},
   "source": [
    "### Results of the model\n",
    "\n",
    "In order to mesure the capacity of the model to get the semantical meaning of the sentence we did this validation. We get some sentenes from the test set, then do an inference through the model and do a retrive with topk as 5, 3 and 1. And check if the title of the searched sentence is in the results.\n",
    "\n",
    "- With 100 samples:\n",
    "\n",
    "    Top 5 accuracy: 51%\n",
    "\n",
    "    Top 3 accuracy: 46%\n",
    "\n",
    "    Top 1 accuracy: 44%\n",
    "\n",
    "<br>\n",
    "\n",
    "- With 300 samples:\n",
    "\n",
    "    Top 5 accuracy: 57,3%\n",
    "\n",
    "    Top 3 accuracy: 52,6%\n",
    "\n",
    "    Top 1 accuracy: 48,6%\n",
    "\n",
    "<br>\n",
    "\n",
    "- With 500 samples:\n",
    "\n",
    "    Top 5 accuracy: 58,8%\n",
    "\n",
    "    Top 3 accuracy: 54%\n",
    "\n",
    "    Top 1 accuracy: 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfd40ec",
   "metadata": {},
   "source": [
    "# Conclusions & further exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "556ffd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distil BERT vs Custom BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fff3d2e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28275a2a",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f510feb2",
   "metadata": {},
   "source": [
    "[1]: Sanh, Victor, et al. \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.\" arXiv preprint [arXiv:1910.01108](https://arxiv.org/abs/1910.01108) (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4040a7cd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "286.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
